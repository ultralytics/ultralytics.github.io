{"config":{"indexing":"full","lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"Introduction To get started right now check out the Quick Start Guide What is YOLOv5 YOLO an acronym for 'You only look once', is an object detection algorithm that divides images into a grid system. Each cell in the grid is responsible for detecting objects within itself. YOLO is one of the most famous object detection algorithms due to its speed and accuracy. The History of YOLO YOLOv5 Shortly after the release of YOLOv4 Glenn Jocher introduced YOLOv5 using the Pytorch framework. The open source code is available on GitHub Author: Glenn Jocher Released: 18 May 2020 YOLOv4 With the original authors work on YOLO coming to a standstill, YOLOv4 was released by Alexey Bochoknovskiy, Chien-Yao Wang, and Hong-Yuan Mark Liao. The paper was titled YOLOv4: Optimal Speed and Accuracy of Object Detection Author: Alexey Bochoknovskiy , Chien-Yao Wang , and Hong-Yuan Mark Liao Released: 23 April 2020 YOLOv3 YOLOv3 improved on the YOLOv2 paper and both Joseph Redmon and Ali Farhadi, the original authors, contributed. Together they published YOLOv3: An Incremental Improvement The original YOLO papers were are hosted here Author: Joseph Redmon and Ali Farhadi Released: 8 Apr 2018 YOLOv2 YOLOv2 was a joint endevor by Joseph Redmon the original author of YOLO and Ali Farhadi. Together they published YOLO9000:Better, Faster, Stronger Author: Joseph Redmon and Ali Farhadi Released: 25 Dec 2016 YOLOv1 YOLOv1 was released as a research paper by Joseph Redmon. The paper was titled You Only Look Once: Unified, Real-Time Object Detection Author: Joseph Redmon Released: 8 Jun 2015","title":"Introduction"},{"location":"#introduction","text":"To get started right now check out the Quick Start Guide","title":"Introduction"},{"location":"#what-is-yolov5","text":"YOLO an acronym for 'You only look once', is an object detection algorithm that divides images into a grid system. Each cell in the grid is responsible for detecting objects within itself. YOLO is one of the most famous object detection algorithms due to its speed and accuracy.","title":"What is YOLOv5"},{"location":"#the-history-of-yolo","text":"","title":"The History of YOLO"},{"location":"#yolov5","text":"Shortly after the release of YOLOv4 Glenn Jocher introduced YOLOv5 using the Pytorch framework. The open source code is available on GitHub Author: Glenn Jocher Released: 18 May 2020","title":"YOLOv5"},{"location":"#yolov4","text":"With the original authors work on YOLO coming to a standstill, YOLOv4 was released by Alexey Bochoknovskiy, Chien-Yao Wang, and Hong-Yuan Mark Liao. The paper was titled YOLOv4: Optimal Speed and Accuracy of Object Detection Author: Alexey Bochoknovskiy , Chien-Yao Wang , and Hong-Yuan Mark Liao Released: 23 April 2020","title":"YOLOv4"},{"location":"#yolov3","text":"YOLOv3 improved on the YOLOv2 paper and both Joseph Redmon and Ali Farhadi, the original authors, contributed. Together they published YOLOv3: An Incremental Improvement The original YOLO papers were are hosted here Author: Joseph Redmon and Ali Farhadi Released: 8 Apr 2018","title":"YOLOv3"},{"location":"#yolov2","text":"YOLOv2 was a joint endevor by Joseph Redmon the original author of YOLO and Ali Farhadi. Together they published YOLO9000:Better, Faster, Stronger Author: Joseph Redmon and Ali Farhadi Released: 25 Dec 2016","title":"YOLOv2"},{"location":"#yolov1","text":"YOLOv1 was released as a research paper by Joseph Redmon. The paper was titled You Only Look Once: Unified, Real-Time Object Detection Author: Joseph Redmon Released: 8 Jun 2015","title":"YOLOv1"},{"location":"quick-start/","text":"Getting Started Requirements You will need Python >= 3.8 and PIP in order to follow this guide. The rest of the requirements are listed in './requirements.txt' * If you have mutliple versions of python installed, ensure you are using the correct one Installation Clone the repository $ git clone https://github.com/ultralytics/yolov5.git Enter the repository root directory $ cd yolov5 Install the required packages from your cloned repository root directory $ pip install -r requirements.txt Packaged Environments For a quick and hassle free setup YOLOv5 has been packaged with all dependencies* for the following environments *including CUDA / CUDNN , Python and PyTorch Google Colab and Kaggle notebooks with free GPU: Google Cloud Deep Learning VM. See GCP Quickstart Guide Amazon Deep Learning AMI. See AWS Quickstart Guide Docker Image . See Docker Quickstart Guide Inference - Detect Objects From Your Cloned Repository To get started with object detection using the latest YOLO models , run this command from your repository root directory. Results are saved to './runs/detect' $ python detect.py --source OPTION Replace OPTION with your selection, to detect from: Webcam : (OPTION = 0) For live object detection from your connected webcam Image : (OPTION = filename.jpg) Create a copy of the image with an object detection overlay Video : (OPTION = filename.mp4) Create a copy of the video with an object detection overlay Directory : (OPTION = directory_name/) Create a copy of all file with an object detection overlay Global File Type (OPTION = directory_name/*.jpg) Create a copy of all file with an object detection overlay RTSP stream : (OPTION = rtsp://170.93.143.139/rtplive/470011e600ef003a004ee33696235daa) For live object detection from a stream RTMP stream : (OPTION = rtmp://192.168.1.105/live/test) For live object detection from a stream HTTP stream : (OPTION = http://112.50.243.8/PLTV/88888888/224/3221225900/1.m3u8) For live object detection from a stream The following file formats are currently supported: Images: bmp, jpg, jpeg, png, tif, tiff, dng, webp, mpo Videos: mov, avi, mp4, mpg, mpeg, m4v, wmv, mkv From PyTorch Hub Inference can be run directly from PyTorch Hub without cloning the repository. The necesary files will be downloaded into your temporary directory. Here is an example script that uses the latest YOLOv5s model and the repositories example images. import torch # Model model = torch.hub.load('ultralytics/yolov5', 'yolov5s') # Images dir = 'https://github.com/ultralytics/yolov5/raw/master/data/images/' imgs = [dir + f for f in ('zidane.jpg', 'bus.jpg')] # batch of images # Inference results = model(imgs) results.print() # or .show(), .save()","title":"Quick Start"},{"location":"quick-start/#getting-started","text":"","title":"Getting Started"},{"location":"quick-start/#requirements","text":"You will need Python >= 3.8 and PIP in order to follow this guide. The rest of the requirements are listed in './requirements.txt' * If you have mutliple versions of python installed, ensure you are using the correct one","title":"Requirements"},{"location":"quick-start/#installation","text":"Clone the repository $ git clone https://github.com/ultralytics/yolov5.git Enter the repository root directory $ cd yolov5 Install the required packages from your cloned repository root directory $ pip install -r requirements.txt","title":"Installation"},{"location":"quick-start/#packaged-environments","text":"For a quick and hassle free setup YOLOv5 has been packaged with all dependencies* for the following environments *including CUDA / CUDNN , Python and PyTorch Google Colab and Kaggle notebooks with free GPU: Google Cloud Deep Learning VM. See GCP Quickstart Guide Amazon Deep Learning AMI. See AWS Quickstart Guide Docker Image . See Docker Quickstart Guide","title":"Packaged Environments"},{"location":"quick-start/#inference-detect-objects","text":"","title":"Inference - Detect Objects"},{"location":"quick-start/#from-your-cloned-repository","text":"To get started with object detection using the latest YOLO models , run this command from your repository root directory. Results are saved to './runs/detect' $ python detect.py --source OPTION Replace OPTION with your selection, to detect from: Webcam : (OPTION = 0) For live object detection from your connected webcam Image : (OPTION = filename.jpg) Create a copy of the image with an object detection overlay Video : (OPTION = filename.mp4) Create a copy of the video with an object detection overlay Directory : (OPTION = directory_name/) Create a copy of all file with an object detection overlay Global File Type (OPTION = directory_name/*.jpg) Create a copy of all file with an object detection overlay RTSP stream : (OPTION = rtsp://170.93.143.139/rtplive/470011e600ef003a004ee33696235daa) For live object detection from a stream RTMP stream : (OPTION = rtmp://192.168.1.105/live/test) For live object detection from a stream HTTP stream : (OPTION = http://112.50.243.8/PLTV/88888888/224/3221225900/1.m3u8) For live object detection from a stream The following file formats are currently supported: Images: bmp, jpg, jpeg, png, tif, tiff, dng, webp, mpo Videos: mov, avi, mp4, mpg, mpeg, m4v, wmv, mkv","title":"From Your Cloned Repository"},{"location":"quick-start/#from-pytorch-hub","text":"Inference can be run directly from PyTorch Hub without cloning the repository. The necesary files will be downloaded into your temporary directory. Here is an example script that uses the latest YOLOv5s model and the repositories example images. import torch # Model model = torch.hub.load('ultralytics/yolov5', 'yolov5s') # Images dir = 'https://github.com/ultralytics/yolov5/raw/master/data/images/' imgs = [dir + f for f in ('zidane.jpg', 'bus.jpg')] # batch of images # Inference results = model(imgs) results.print() # or .show(), .save()","title":"From PyTorch Hub"},{"location":"FAQ/augmentation/","text":"YOLOv5 \ud83d\ude80 applies online imagespace and colorspace augmentations in the trainloader (but not the val_loader) to present a new and unique augmented Mosaic (original image + 3 random images) each time an image is loaded for training. Images are never presented twice in the same way . Augmentation Hyperparameters The hyperparameters used to define these augmentations are in your hyperparameter file (default data/hyp.scratch.yaml ) defined when training: python train.py --hyp hyp.scratch-low.yaml https://github.com/ultralytics/yolov5/blob/b94b59e199047aa8bf2cdd4401ae9f5f42b929e6/data/hyps/hyp.scratch-low.yaml#L6-L34 Augmentation Previews You can view the effect of your augmentation policy in your train_batch*.jpg images once training starts. These images will be in your train logging directory, typically yolov5/runs/train/exp : train_batch0.jpg shows train batch 0 mosaics and labels: YOLOv5 Albumentations Integration YOLOv5 \ud83d\ude80 is now fully integrated with Albumentations , a popular open-source image augmentation package. Now you can train the world's best Vision AI models even better with custom Albumentations \ud83d\ude03! PR https://github.com/ultralytics/yolov5/pull/3882 implements this integration, which will automatically apply Albumentations transforms during YOLOv5 training if albumentations>=1.0.3 is installed in your environment. See https://github.com/ultralytics/yolov5/pull/3882 for full details. Example train_batch0.jpg on COCO128 dataset with Blur, MedianBlur and ToGray. See the YOLOv5 Notebooks to reproduce: Good luck \ud83c\udf40 and let us know if you have any other questions!","title":"Augmentation"},{"location":"FAQ/augmentation/#augmentation-hyperparameters","text":"The hyperparameters used to define these augmentations are in your hyperparameter file (default data/hyp.scratch.yaml ) defined when training: python train.py --hyp hyp.scratch-low.yaml https://github.com/ultralytics/yolov5/blob/b94b59e199047aa8bf2cdd4401ae9f5f42b929e6/data/hyps/hyp.scratch-low.yaml#L6-L34","title":"Augmentation Hyperparameters"},{"location":"FAQ/augmentation/#augmentation-previews","text":"You can view the effect of your augmentation policy in your train_batch*.jpg images once training starts. These images will be in your train logging directory, typically yolov5/runs/train/exp : train_batch0.jpg shows train batch 0 mosaics and labels:","title":"Augmentation Previews"},{"location":"FAQ/augmentation/#yolov5-albumentations-integration","text":"YOLOv5 \ud83d\ude80 is now fully integrated with Albumentations , a popular open-source image augmentation package. Now you can train the world's best Vision AI models even better with custom Albumentations \ud83d\ude03! PR https://github.com/ultralytics/yolov5/pull/3882 implements this integration, which will automatically apply Albumentations transforms during YOLOv5 training if albumentations>=1.0.3 is installed in your environment. See https://github.com/ultralytics/yolov5/pull/3882 for full details. Example train_batch0.jpg on COCO128 dataset with Blur, MedianBlur and ToGray. See the YOLOv5 Notebooks to reproduce: Good luck \ud83c\udf40 and let us know if you have any other questions!","title":"YOLOv5 Albumentations Integration"},{"location":"environments/AWS-Quickstart/","text":"This quickstart guide helps new users run YOLOv5 \ud83d\ude80 on an Amazon Web Services (AWS) Deep Learning instance \u2b50. AWS offers a Free Tier and a credit program to get started quickly and affordably. Other quickstart options for YOLOv5 include our Colab Notebook , GCP Deep Learning VM and our Docker image at https://hub.docker.com/r/ultralytics/yolov5 . 1. Console Sign-in Create and account or sign-in to the AWS console at https://aws.amazon.com/console/ and then select the EC2 service. 2. Launch Instance In the EC2 part of the AWS console, click the Launch instance button. Choose an Amazon Machine Image (AMI) Enter 'Deep Learning' in the search field and select the most recent Ubuntu Deep Learning AMI (recommended), or select an alternative Deep Learning AMI. See Choosing Your DLAMI for more information on selecting an AMI. Select an Instance Type A GPU instance is recommended for most deep learning purposes. Training new models will be faster on a GPU instance than a CPU instance. You can scale sub-linearly when you have multi-GPU instances or if you use distributed training across many instances with GPUs. To set up distributed training, see Distrbuted Training . Note: The size of your model should be a factor in selecting an instance. If your model exceeds an instance's available RAM, select a different instance type with enough memory for your application. Amazon EC2 P3 Instances have up to 8 NVIDIA Tesla V100 GPUs. Amazon EC2 P2 Instances have up to 16 NVIDIA NVIDIA K80 GPUs. Amazon EC2 G3 Instances have up to 4 NVIDIA Tesla M60 GPUs. Amazon EC2 G4 Instances have up to 4 NVIDIA T4 GPUs. Amazon EC2 P4 Instances have up to 8 NVIDIA Tesla A100 GPUs. Check out EC2 Instance Types and choose Accelerated Computing to see the different GPU instance options. DLAMI instances provide tooling to monitor and optimize your GPU processes. For more information on overseeing your GPU processes, see GPU Monitoring and Optimization . For pricing see On Demand Pricing and Spot Pricing . Configure Instance Details Amazon EC2 Spot Instances let you take advantage of unused EC2 capacity in the AWS cloud. Spot Instances are available at up to a 70% discount compared to On-Demand prices. We recommend a persistent spot instance, which will save your data and restart automatically when spot instance availability returns after spot instance termination. For full-price On-Demand instances leave these settings to their default values. Complete Steps 4-7 to finalize your instance hardware and security settings and then launch the instance. 3. Connect to Instance Select the check box next to your running instance, and then click connect. You can copy paste the SSH terminal command into a terminal of your choice to connect to your instance. 4. Run YOLOv5 \ud83d\ude80 Once you have logged in to your instance, clone this repo and install requirements.txt dependencies, including Python>=3.8 and PyTorch>=1.7 . $ git clone https://github.com/ultralytics/yolov5 # clone repo $ cd yolov5 $ pip install -r requirements.txt # install dependencies Then get started training, testing and detecting! $ python train.py # train a model $ python test.py --weights yolov5s.pt # test a model for Precision, Recall and mAP $ python detect.py --weights yolov5s.pt --source path/to/images # run inference on images and videos Optional Extras Add 64GB of swap memory (to --cache large datasets). sudo fallocate -l 64G /swapfile sudo chmod 600 /swapfile sudo mkswap /swapfile sudo swapon /swapfile free -h # check memory","title":"Amazon Web Services"},{"location":"environments/AWS-Quickstart/#1-console-sign-in","text":"Create and account or sign-in to the AWS console at https://aws.amazon.com/console/ and then select the EC2 service.","title":"1. Console Sign-in"},{"location":"environments/AWS-Quickstart/#2-launch-instance","text":"In the EC2 part of the AWS console, click the Launch instance button.","title":"2. Launch Instance"},{"location":"environments/AWS-Quickstart/#choose-an-amazon-machine-image-ami","text":"Enter 'Deep Learning' in the search field and select the most recent Ubuntu Deep Learning AMI (recommended), or select an alternative Deep Learning AMI. See Choosing Your DLAMI for more information on selecting an AMI.","title":"Choose an Amazon Machine Image (AMI)"},{"location":"environments/AWS-Quickstart/#select-an-instance-type","text":"A GPU instance is recommended for most deep learning purposes. Training new models will be faster on a GPU instance than a CPU instance. You can scale sub-linearly when you have multi-GPU instances or if you use distributed training across many instances with GPUs. To set up distributed training, see Distrbuted Training . Note: The size of your model should be a factor in selecting an instance. If your model exceeds an instance's available RAM, select a different instance type with enough memory for your application. Amazon EC2 P3 Instances have up to 8 NVIDIA Tesla V100 GPUs. Amazon EC2 P2 Instances have up to 16 NVIDIA NVIDIA K80 GPUs. Amazon EC2 G3 Instances have up to 4 NVIDIA Tesla M60 GPUs. Amazon EC2 G4 Instances have up to 4 NVIDIA T4 GPUs. Amazon EC2 P4 Instances have up to 8 NVIDIA Tesla A100 GPUs. Check out EC2 Instance Types and choose Accelerated Computing to see the different GPU instance options. DLAMI instances provide tooling to monitor and optimize your GPU processes. For more information on overseeing your GPU processes, see GPU Monitoring and Optimization . For pricing see On Demand Pricing and Spot Pricing .","title":"Select an Instance Type"},{"location":"environments/AWS-Quickstart/#configure-instance-details","text":"Amazon EC2 Spot Instances let you take advantage of unused EC2 capacity in the AWS cloud. Spot Instances are available at up to a 70% discount compared to On-Demand prices. We recommend a persistent spot instance, which will save your data and restart automatically when spot instance availability returns after spot instance termination. For full-price On-Demand instances leave these settings to their default values. Complete Steps 4-7 to finalize your instance hardware and security settings and then launch the instance.","title":"Configure Instance Details"},{"location":"environments/AWS-Quickstart/#3-connect-to-instance","text":"Select the check box next to your running instance, and then click connect. You can copy paste the SSH terminal command into a terminal of your choice to connect to your instance.","title":"3. Connect to Instance"},{"location":"environments/AWS-Quickstart/#4-run-yolov5","text":"Once you have logged in to your instance, clone this repo and install requirements.txt dependencies, including Python>=3.8 and PyTorch>=1.7 . $ git clone https://github.com/ultralytics/yolov5 # clone repo $ cd yolov5 $ pip install -r requirements.txt # install dependencies Then get started training, testing and detecting! $ python train.py # train a model $ python test.py --weights yolov5s.pt # test a model for Precision, Recall and mAP $ python detect.py --weights yolov5s.pt --source path/to/images # run inference on images and videos","title":"4. Run YOLOv5 \ud83d\ude80"},{"location":"environments/AWS-Quickstart/#optional-extras","text":"Add 64GB of swap memory (to --cache large datasets). sudo fallocate -l 64G /swapfile sudo chmod 600 /swapfile sudo mkswap /swapfile sudo swapon /swapfile free -h # check memory","title":"Optional Extras"},{"location":"environments/Docker-Quickstart/","text":"To get started with YOLOv5 \ud83d\ude80 in a Docker image follow the instructions below. Other quickstart options for YOLOv5 include our Colab Notebook and a GCP Deep Learning VM . 1. Install Docker and Nvidia-Docker Docker images come with all dependencies preinstalled, however Docker itself requires installation, and relies of nvidia driver installations in order to interact properly with local GPU resources. The requirements are: - Nvidia Driver >= 455.23 https://www.nvidia.com/Download/index.aspx - Nvidia-Docker https://github.com/NVIDIA/nvidia-docker - Docker Engine - CE >= 19.03 https://docs.docker.com/install/ 2. Pull Image The Ultralytics YOLOv5 DockerHub is https://hub.docker.com/r/ultralytics/yolov5 . Docker Autobuild is used to automatically build images from the latest repository commits, so the ultralytics/yolov5:latest image hosted on the DockerHub will always be in sync with the most recent repository commit . To pull this image: sudo docker pull ultralytics/yolov5:latest 3. Run Container Run an interactive instance of this image (called a \"container\") using -it : sudo docker run --ipc=host -it ultralytics/yolov5:latest Run a container with local file access (like COCO training data in /coco ) using -v : sudo docker run --ipc=host -it -v \"$(pwd)\"/coco:/usr/src/coco ultralytics/yolov5:latest Run a container with GPU access using --gpus all : sudo docker run --ipc=host --gpus all -it ultralytics/yolov5:latest 4. Run Commands Run commands from within the running Docker container, i.e.: $ python train.py # train a model $ python test.py --weights yolov5s.pt # test a model for Precision, Recall and mAP $ python detect.py --weights yolov5s.pt --source path/to/images # run inference on images and videos","title":"Docker"},{"location":"environments/Docker-Quickstart/#1-install-docker-and-nvidia-docker","text":"Docker images come with all dependencies preinstalled, however Docker itself requires installation, and relies of nvidia driver installations in order to interact properly with local GPU resources. The requirements are: - Nvidia Driver >= 455.23 https://www.nvidia.com/Download/index.aspx - Nvidia-Docker https://github.com/NVIDIA/nvidia-docker - Docker Engine - CE >= 19.03 https://docs.docker.com/install/","title":"1. Install Docker and Nvidia-Docker"},{"location":"environments/Docker-Quickstart/#2-pull-image","text":"The Ultralytics YOLOv5 DockerHub is https://hub.docker.com/r/ultralytics/yolov5 . Docker Autobuild is used to automatically build images from the latest repository commits, so the ultralytics/yolov5:latest image hosted on the DockerHub will always be in sync with the most recent repository commit . To pull this image: sudo docker pull ultralytics/yolov5:latest","title":"2. Pull Image"},{"location":"environments/Docker-Quickstart/#3-run-container","text":"Run an interactive instance of this image (called a \"container\") using -it : sudo docker run --ipc=host -it ultralytics/yolov5:latest Run a container with local file access (like COCO training data in /coco ) using -v : sudo docker run --ipc=host -it -v \"$(pwd)\"/coco:/usr/src/coco ultralytics/yolov5:latest Run a container with GPU access using --gpus all : sudo docker run --ipc=host --gpus all -it ultralytics/yolov5:latest","title":"3. Run Container"},{"location":"environments/Docker-Quickstart/#4-run-commands","text":"Run commands from within the running Docker container, i.e.: $ python train.py # train a model $ python test.py --weights yolov5s.pt # test a model for Precision, Recall and mAP $ python detect.py --weights yolov5s.pt --source path/to/images # run inference on images and videos","title":"4. Run Commands"},{"location":"environments/GCP-Quickstart/","text":"This quickstart guide helps new users run YOLOv5 \ud83d\ude80 on a Google Cloud Platform (GCP) Deep Learning Virtual Machine (VM) \u2b50. New GCP users are eligible for a $300 free credit offer . Other quickstart options for YOLOv5 include our Colab Notebook and our Docker image at https://hub.docker.com/r/ultralytics/yolov5 . 1. Create VM Select a Deep Learning VM from the GCP marketplace , select an n1-standard-8 instance (with 8 vCPUs and 30 GB memory), add a GPU of your choice, check 'Install NVIDIA GPU driver automatically on first startup?', and select a 300 GB SSD Persistent Disk for sufficient I/O speed, then click 'Deploy'. All dependencies are included in the preinstalled Anaconda Python environment. 2. Setup VM Clone this repo and install requirements.txt dependencies, including Python>=3.8 and PyTorch>=1.7 . $ git clone https://github.com/ultralytics/yolov5 # clone repo $ cd yolov5 $ pip install -r requirements.txt # install dependencies 3. Run Commands $ python train.py # train a model $ python test.py --weights yolov5s.pt # test a model for Precision, Recall and mAP $ python detect.py --weights yolov5s.pt --source path/to/images # run inference on images and videos Optional Extras Add 64GB of swap memory (to --cache large datasets). sudo fallocate -l 64G /swapfile sudo chmod 600 /swapfile sudo mkswap /swapfile sudo swapon /swapfile free -h # check memory Mount local SSD lsblk sudo mkfs.ext4 -F /dev/nvme0n1 sudo mkdir -p /mnt/disks/nvme0n1 sudo mount /dev/nvme0n1 /mnt/disks/nvme0n1 sudo chmod a+w /mnt/disks/nvme0n1 cp -r coco /mnt/disks/nvme0n1","title":"Google Cloud Platform"},{"location":"environments/GCP-Quickstart/#1-create-vm","text":"Select a Deep Learning VM from the GCP marketplace , select an n1-standard-8 instance (with 8 vCPUs and 30 GB memory), add a GPU of your choice, check 'Install NVIDIA GPU driver automatically on first startup?', and select a 300 GB SSD Persistent Disk for sufficient I/O speed, then click 'Deploy'. All dependencies are included in the preinstalled Anaconda Python environment.","title":"1. Create VM"},{"location":"environments/GCP-Quickstart/#2-setup-vm","text":"Clone this repo and install requirements.txt dependencies, including Python>=3.8 and PyTorch>=1.7 . $ git clone https://github.com/ultralytics/yolov5 # clone repo $ cd yolov5 $ pip install -r requirements.txt # install dependencies","title":"2. Setup VM"},{"location":"environments/GCP-Quickstart/#3-run-commands","text":"$ python train.py # train a model $ python test.py --weights yolov5s.pt # test a model for Precision, Recall and mAP $ python detect.py --weights yolov5s.pt --source path/to/images # run inference on images and videos","title":"3. Run Commands"},{"location":"environments/GCP-Quickstart/#optional-extras","text":"Add 64GB of swap memory (to --cache large datasets). sudo fallocate -l 64G /swapfile sudo chmod 600 /swapfile sudo mkswap /swapfile sudo swapon /swapfile free -h # check memory Mount local SSD lsblk sudo mkfs.ext4 -F /dev/nvme0n1 sudo mkdir -p /mnt/disks/nvme0n1 sudo mount /dev/nvme0n1 /mnt/disks/nvme0n1 sudo chmod a+w /mnt/disks/nvme0n1 cp -r coco /mnt/disks/nvme0n1","title":"Optional Extras"},{"location":"tutorials/hyperparameter-evolution/","text":"\ud83d\udcda This guide explains hyperparameter evolution for YOLOv5 \ud83d\ude80. Hyperparameter evolution is a method of Hyperparameter Optimization using a Genetic Algorithm (GA) for optimization. Hyperparameters in ML control various aspects of training, and finding optimal values for them can be a challenge. Traditional methods like grid searches can quickly become intractable due to 1) the high dimensional search space 2) unknown correlations among the dimensions, and 3) expensive nature of evaluating the fitness at each point, making GA a suitable candidate for hyperparameter searches. Before You Start Clone this repo and install requirements.txt dependencies, including Python>=3.8 and PyTorch>=1.7 . $ git clone https://github.com/ultralytics/yolov5 # clone repo $ cd yolov5 $ pip install -r requirements.txt # install dependencies 1. Initialize Hyperparameters YOLOv5 has about 25 hyperparameters used for various training settings. These are defined in yaml files in the /data directory. Better initial guesses will produce better final results, so it is important to initialize these values properly before evolving. If in doubt, simply use the default values, which are optimized for YOLOv5 COCO training from scratch. https://github.com/ultralytics/yolov5/blob/3bb414890a253bb1a269fb81cc275d11c8fffa72/data/hyp.scratch.yaml#L1-L33 2. Define Fitness Fitness is the value we seek to maximize. In YOLOv5 we have defined a default fitness function as a weighted combination of metrics: mAP@0.5 contributes 10% of the weight and mAP@0.5:0.95 contributes the remaining 90%. You may adjust these as you see fit or use the default fitness definition. https://github.com/ultralytics/yolov5/blob/c5d233189729a9e7e25d3aa2d347aed02b545d30/utils/general.py#L917-L921 3. Evolve Evolution is performed about a base scenario which we seek to improve upon. The base scenario in this example is finetuning COCO128 for 10 epochs using pretrained YOLOv5s. The base scenario training command is: python train.py --epochs 10 --data coco128.yaml --weights yolov5s.pt --cache To evolve hyperparameters specific to this scenario , starting from our initial values defined in Section 1. , and maximizing the fitness defined in Section 2. , append --evolve : # Single-GPU python train.py --epochs 10 --data coco128.yaml --weights yolov5s.pt --cache --evolve # Multi-GPU for i in 0 1 2 3; do nohup python train.py --epochs 10 --data coco128.yaml --weights yolov5s.pt --cache --evolve --device $i > evolve_gpu_$i.log & done # Multi-GPU bash while (not recommended) for i in 0 1 2 3; do nohup \"$(while true; do python train.py ... --evolve --device $i; done)\" > evolve_gpu_$i.log & done The default evolution settings will run the base scenario 300 times, i.e. for 300 generations: https://github.com/ultralytics/yolov5/blob/c5d233189729a9e7e25d3aa2d347aed02b545d30/train.py#L497 The main genetic operators are crossover and mutation . In this work mutation is used, with a 90% probability and a 0.04 variance to create new offspring based on a combination of the best parents from all previous generations. Results are tracked in yolov5/evolve.txt , and the highest fitness offspring is saved every generation as yolov5/runs/evolve/hyp_evolved.yaml : # Hyperparameter Evolution Results # Generations: 1000 # P R mAP.5 mAP.5:.95 box obj cls # Metrics: 0.4761 0.79 0.763 0.4951 0.01926 0.03286 0.003559 lr0: 0.01 # initial learning rate (SGD=1E-2, Adam=1E-3) lrf: 0.2 # final OneCycleLR learning rate (lr0 * lrf) momentum: 0.937 # SGD momentum/Adam beta1 weight_decay: 0.0005 # optimizer weight decay 5e-4 warmup_epochs: 3.0 # warmup epochs (fractions ok) warmup_momentum: 0.8 # warmup initial momentum warmup_bias_lr: 0.1 # warmup initial bias lr box: 0.05 # box loss gain cls: 0.5 # cls loss gain cls_pw: 1.0 # cls BCELoss positive_weight obj: 1.0 # obj loss gain (scale with pixels) obj_pw: 1.0 # obj BCELoss positive_weight iou_t: 0.20 # IoU training threshold anchor_t: 4.0 # anchor-multiple threshold anchors: 0 # anchors per output grid (0 to ignore) fl_gamma: 0.0 # focal loss gamma (efficientDet default gamma=1.5) hsv_h: 0.015 # image HSV-Hue augmentation (fraction) hsv_s: 0.7 # image HSV-Saturation augmentation (fraction) hsv_v: 0.4 # image HSV-Value augmentation (fraction) degrees: 0.0 # image rotation (+/- deg) translate: 0.1 # image translation (+/- fraction) scale: 0.5 # image scale (+/- gain) shear: 0.0 # image shear (+/- deg) perspective: 0.0 # image perspective (+/- fraction), range 0-0.001 flipud: 0.0 # image flip up-down (probability) fliplr: 0.5 # image flip left-right (probability) mosaic: 1.0 # image mosaic (probability) mixup: 0.0 # image mixup (probability) We recommend a minimum of 300 generations of evolution for best results. Note that evolution is generally expensive and time consuming , as the base scenario is trained hundreds of times, possibly requiring hundreds or thousands of GPU hours. 4. Visualize Results are saved as yolov5/evolve.png , with one plot per hyperparameter. Values are on the x axis and fitness on the y axis. Yellow indicates higher concentrations. Vertical lines indicate that a parameter has been disabled and does not mutate. This is user selectable in the meta dictionary in train.py, and is useful for fixing parameters and preventing them from evolving. Environments YOLOv5 may be run in any of the following up-to-date verified environments (with all dependencies including CUDA / CUDNN , Python and PyTorch preinstalled): Google Colab and Kaggle notebooks with free GPU: Google Cloud Deep Learning VM. See GCP Quickstart Guide Amazon Deep Learning AMI. See AWS Quickstart Guide Docker Image . See Docker Quickstart Guide Status If this badge is green, all YOLOv5 GitHub Actions Continuous Integration (CI) tests are currently passing. CI tests verify correct operation of YOLOv5 training ( train.py ), testing ( test.py ), inference ( detect.py ) and export ( export.py ) on MacOS, Windows, and Ubuntu every 24 hours and on every commit.","title":"Hyperparameter Evolution"},{"location":"tutorials/hyperparameter-evolution/#before-you-start","text":"Clone this repo and install requirements.txt dependencies, including Python>=3.8 and PyTorch>=1.7 . $ git clone https://github.com/ultralytics/yolov5 # clone repo $ cd yolov5 $ pip install -r requirements.txt # install dependencies","title":"Before You Start"},{"location":"tutorials/hyperparameter-evolution/#1-initialize-hyperparameters","text":"YOLOv5 has about 25 hyperparameters used for various training settings. These are defined in yaml files in the /data directory. Better initial guesses will produce better final results, so it is important to initialize these values properly before evolving. If in doubt, simply use the default values, which are optimized for YOLOv5 COCO training from scratch. https://github.com/ultralytics/yolov5/blob/3bb414890a253bb1a269fb81cc275d11c8fffa72/data/hyp.scratch.yaml#L1-L33","title":"1. Initialize Hyperparameters"},{"location":"tutorials/hyperparameter-evolution/#2-define-fitness","text":"Fitness is the value we seek to maximize. In YOLOv5 we have defined a default fitness function as a weighted combination of metrics: mAP@0.5 contributes 10% of the weight and mAP@0.5:0.95 contributes the remaining 90%. You may adjust these as you see fit or use the default fitness definition. https://github.com/ultralytics/yolov5/blob/c5d233189729a9e7e25d3aa2d347aed02b545d30/utils/general.py#L917-L921","title":"2. Define Fitness"},{"location":"tutorials/hyperparameter-evolution/#3-evolve","text":"Evolution is performed about a base scenario which we seek to improve upon. The base scenario in this example is finetuning COCO128 for 10 epochs using pretrained YOLOv5s. The base scenario training command is: python train.py --epochs 10 --data coco128.yaml --weights yolov5s.pt --cache To evolve hyperparameters specific to this scenario , starting from our initial values defined in Section 1. , and maximizing the fitness defined in Section 2. , append --evolve : # Single-GPU python train.py --epochs 10 --data coco128.yaml --weights yolov5s.pt --cache --evolve # Multi-GPU for i in 0 1 2 3; do nohup python train.py --epochs 10 --data coco128.yaml --weights yolov5s.pt --cache --evolve --device $i > evolve_gpu_$i.log & done # Multi-GPU bash while (not recommended) for i in 0 1 2 3; do nohup \"$(while true; do python train.py ... --evolve --device $i; done)\" > evolve_gpu_$i.log & done The default evolution settings will run the base scenario 300 times, i.e. for 300 generations: https://github.com/ultralytics/yolov5/blob/c5d233189729a9e7e25d3aa2d347aed02b545d30/train.py#L497 The main genetic operators are crossover and mutation . In this work mutation is used, with a 90% probability and a 0.04 variance to create new offspring based on a combination of the best parents from all previous generations. Results are tracked in yolov5/evolve.txt , and the highest fitness offspring is saved every generation as yolov5/runs/evolve/hyp_evolved.yaml : # Hyperparameter Evolution Results # Generations: 1000 # P R mAP.5 mAP.5:.95 box obj cls # Metrics: 0.4761 0.79 0.763 0.4951 0.01926 0.03286 0.003559 lr0: 0.01 # initial learning rate (SGD=1E-2, Adam=1E-3) lrf: 0.2 # final OneCycleLR learning rate (lr0 * lrf) momentum: 0.937 # SGD momentum/Adam beta1 weight_decay: 0.0005 # optimizer weight decay 5e-4 warmup_epochs: 3.0 # warmup epochs (fractions ok) warmup_momentum: 0.8 # warmup initial momentum warmup_bias_lr: 0.1 # warmup initial bias lr box: 0.05 # box loss gain cls: 0.5 # cls loss gain cls_pw: 1.0 # cls BCELoss positive_weight obj: 1.0 # obj loss gain (scale with pixels) obj_pw: 1.0 # obj BCELoss positive_weight iou_t: 0.20 # IoU training threshold anchor_t: 4.0 # anchor-multiple threshold anchors: 0 # anchors per output grid (0 to ignore) fl_gamma: 0.0 # focal loss gamma (efficientDet default gamma=1.5) hsv_h: 0.015 # image HSV-Hue augmentation (fraction) hsv_s: 0.7 # image HSV-Saturation augmentation (fraction) hsv_v: 0.4 # image HSV-Value augmentation (fraction) degrees: 0.0 # image rotation (+/- deg) translate: 0.1 # image translation (+/- fraction) scale: 0.5 # image scale (+/- gain) shear: 0.0 # image shear (+/- deg) perspective: 0.0 # image perspective (+/- fraction), range 0-0.001 flipud: 0.0 # image flip up-down (probability) fliplr: 0.5 # image flip left-right (probability) mosaic: 1.0 # image mosaic (probability) mixup: 0.0 # image mixup (probability) We recommend a minimum of 300 generations of evolution for best results. Note that evolution is generally expensive and time consuming , as the base scenario is trained hundreds of times, possibly requiring hundreds or thousands of GPU hours.","title":"3. Evolve"},{"location":"tutorials/hyperparameter-evolution/#4-visualize","text":"Results are saved as yolov5/evolve.png , with one plot per hyperparameter. Values are on the x axis and fitness on the y axis. Yellow indicates higher concentrations. Vertical lines indicate that a parameter has been disabled and does not mutate. This is user selectable in the meta dictionary in train.py, and is useful for fixing parameters and preventing them from evolving.","title":"4. Visualize"},{"location":"tutorials/hyperparameter-evolution/#environments","text":"YOLOv5 may be run in any of the following up-to-date verified environments (with all dependencies including CUDA / CUDNN , Python and PyTorch preinstalled): Google Colab and Kaggle notebooks with free GPU: Google Cloud Deep Learning VM. See GCP Quickstart Guide Amazon Deep Learning AMI. See AWS Quickstart Guide Docker Image . See Docker Quickstart Guide","title":"Environments"},{"location":"tutorials/hyperparameter-evolution/#status","text":"If this badge is green, all YOLOv5 GitHub Actions Continuous Integration (CI) tests are currently passing. CI tests verify correct operation of YOLOv5 training ( train.py ), testing ( test.py ), inference ( detect.py ) and export ( export.py ) on MacOS, Windows, and Ubuntu every 24 hours and on every commit.","title":"Status"},{"location":"tutorials/model-ensembling/","text":"\ud83d\udcda This guide explains how to use YOLOv5 \ud83d\ude80 model ensembling during testing and inference for improved mAP and Recall. From https://www.sciencedirect.com/topics/computer-science/ensemble-modeling: Ensemble modeling is a process where multiple diverse models are created to predict an outcome, either by using many different modeling algorithms or using different training data sets. The ensemble model then aggregates the prediction of each base model and results in once final prediction for the unseen data. The motivation for using ensemble models is to reduce the generalization error of the prediction. As long as the base models are diverse and independent, the prediction error of the model decreases when the ensemble approach is used. The approach seeks the wisdom of crowds in making a prediction. Even though the ensemble model has multiple base models within the model, it acts and performs as a single model. Before You Start Clone this repo and install requirements.txt dependencies, including Python>=3.8 and PyTorch>=1.7 . git clone https://github.com/ultralytics/yolov5 # clone repo cd yolov5 pip install -r requirements.txt # install requirements.txt Test Normally Before ensembling we want to establish the baseline performance of a single model. This command tests YOLOv5x on COCO val2017 at image size 640 pixels. yolov5x.pt is the largest and most accurate model available. Other options are yolov5s.pt , yolov5m.pt and yolov5l.pt , or you own checkpoint from training a custom dataset ./weights/best.pt . For details on all available models please see our README table . $ python test.py --weights yolov5x.pt --data coco.yaml --img 640 Output: Namespace(augment=False, batch_size=32, conf_thres=0.001, data='./data/coco.yaml', device='', img_size=640, iou_thres=0.65, save_json=True, save_txt=False, single_cls=False, task='val', verbose=False, weights=['yolov5x.pt']) Using CUDA device0 _CudaDeviceProperties(name='Tesla P100-PCIE-16GB', total_memory=16280MB) Fusing layers... Model Summary: 284 layers, 8.89222e+07 parameters, 0 gradients Scanning labels ../coco/labels/val2017.cache (4952 found, 0 missing, 48 empty, 0 duplicate, for 5000 images): 5000it [00:00, 17761.74it/s] Class Images Targets P R mAP@.5 mAP@.5:.95: 100% 157/157 [02:34<00:00, 1.02it/s] all 5e+03 3.63e+04 0.409 0.754 0.669 0.476 Speed: 23.6/1.6/25.2 ms inference/NMS/total per 640x640 image at batch-size 32 COCO mAP with pycocotools... saving detections_val2017__results.json... Average Precision (AP) @[ IoU=0.50:0.95 | area= all | maxDets=100 ] = 0.492 < ---------- baseline mAP Average Precision (AP) @[ IoU=0.50 | area= all | maxDets=100 ] = 0.676 Average Precision (AP) @[ IoU=0.75 | area= all | maxDets=100 ] = 0.534 Average Precision (AP) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.318 Average Precision (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.541 Average Precision (AP) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.633 Average Recall (AR) @[ IoU=0.50:0.95 | area= all | maxDets= 1 ] = 0.376 Average Recall (AR) @[ IoU=0.50:0.95 | area= all | maxDets= 10 ] = 0.616 Average Recall (AR) @[ IoU=0.50:0.95 | area= all | maxDets=100 ] = 0.670 Average Recall (AR) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.493 Average Recall (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.723 Average Recall (AR) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.812 Ensemble Test Multiple pretraind models may be ensembled togethor at test and inference time by simply appending extra models to the --weights argument in any existing test.py or detect.py command. This example tests an ensemble of 2 models togethor: - YOLOv5x - YOLOv5l $ python test.py --weights yolov5x.pt yolov5l.pt --data coco.yaml --img 640 Output: Namespace(augment=False, batch_size=32, conf_thres=0.001, data='./data/coco.yaml', device='', img_size=640, iou_thres=0.65, save_json=True, save_txt=False, single_cls=False, task='val', verbose=False, weights=['yolov5x.pt', 'yolov5l.pt']) Using CUDA device0 _CudaDeviceProperties(name='Tesla P100-PCIE-16GB', total_memory=16280MB) Fusing layers... Model Summary: 284 layers, 8.89222e+07 parameters, 0 gradients # Model 1 Fusing layers... Model Summary: 236 layers, 4.77901e+07 parameters, 0 gradients # Model 2 Ensemble created with ['yolov5x.pt', 'yolov5l.pt'] # Ensemble Notice Scanning labels ../coco/labels/val2017.cache (4952 found, 0 missing, 48 empty, 0 duplicate, for 5000 images): 5000it [00:00, 17883.26it/s] Class Images Targets P R mAP@.5 mAP@.5:.95: 100% 157/157 [03:42<00:00, 1.42s/it] all 5e+03 3.63e+04 0.402 0.764 0.677 0.48 Speed: 37.5/1.4/38.9 ms inference/NMS/total per 640x640 image at batch-size 32 COCO mAP with pycocotools... saving detections_val2017__results.json... Average Precision (AP) @[ IoU=0.50:0.95 | area= all | maxDets=100 ] = 0.496 < ---------- improved mAP Average Precision (AP) @[ IoU=0.50 | area= all | maxDets=100 ] = 0.684 Average Precision (AP) @[ IoU=0.75 | area= all | maxDets=100 ] = 0.538 Average Precision (AP) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.323 Average Precision (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.548 Average Precision (AP) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.633 Average Recall (AR) @[ IoU=0.50:0.95 | area= all | maxDets= 1 ] = 0.377 Average Recall (AR) @[ IoU=0.50:0.95 | area= all | maxDets= 10 ] = 0.615 Average Recall (AR) @[ IoU=0.50:0.95 | area= all | maxDets=100 ] = 0.670 Average Recall (AR) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.495 Average Recall (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.723 Average Recall (AR) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.815 Ensemble Inference Append extra models to the --weights argument to run ensemble inference: $ python detect.py --weights yolov5x.pt yolov5l.pt --img 640 --source ./inference/images/ Output: Namespace(agnostic_nms=False, augment=False, classes=None, conf_thres=0.25, device='', img_size=640, iou_thres=0.45, output='inference/output', save_txt=False, source='./inference/images/', update=False, view_img=False, weights=['yolov5x.pt', 'yolov5l.pt']) Using CUDA device0 _CudaDeviceProperties(name='Tesla P100-PCIE-16GB', total_memory=16280MB) Fusing layers... Model Summary: 284 layers, 8.89222e+07 parameters, 0 gradients # Model 1 Fusing layers... Model Summary: 236 layers, 4.77901e+07 parameters, 0 gradients # Model 2 Ensemble created with ['yolov5x.pt', 'yolov5l.pt'] # Ensemble Notice image 1/2 inference/images/bus.jpg: 640x512 4 persons, 1 bicycles, 1 buss, Done. (0.073s) image 2/2 inference/images/zidane.jpg: 384x640 3 persons, 3 ties, Done. (0.063s) Results saved to inference/output Done. (0.319s) Environments YOLOv5 may be run in any of the following up-to-date verified environments (with all dependencies including CUDA / CUDNN , Python and PyTorch preinstalled): Google Colab and Kaggle notebooks with free GPU: Google Cloud Deep Learning VM. See GCP Quickstart Guide Amazon Deep Learning AMI. See AWS Quickstart Guide Docker Image . See Docker Quickstart Guide Status If this badge is green, all YOLOv5 GitHub Actions Continuous Integration (CI) tests are currently passing. CI tests verify correct operation of YOLOv5 training ( train.py ), testing ( test.py ), inference ( detect.py ) and export ( export.py ) on MacOS, Windows, and Ubuntu every 24 hours and on every commit.","title":"Model Ensembling"},{"location":"tutorials/model-ensembling/#before-you-start","text":"Clone this repo and install requirements.txt dependencies, including Python>=3.8 and PyTorch>=1.7 . git clone https://github.com/ultralytics/yolov5 # clone repo cd yolov5 pip install -r requirements.txt # install requirements.txt","title":"Before You Start"},{"location":"tutorials/model-ensembling/#test-normally","text":"Before ensembling we want to establish the baseline performance of a single model. This command tests YOLOv5x on COCO val2017 at image size 640 pixels. yolov5x.pt is the largest and most accurate model available. Other options are yolov5s.pt , yolov5m.pt and yolov5l.pt , or you own checkpoint from training a custom dataset ./weights/best.pt . For details on all available models please see our README table . $ python test.py --weights yolov5x.pt --data coco.yaml --img 640 Output: Namespace(augment=False, batch_size=32, conf_thres=0.001, data='./data/coco.yaml', device='', img_size=640, iou_thres=0.65, save_json=True, save_txt=False, single_cls=False, task='val', verbose=False, weights=['yolov5x.pt']) Using CUDA device0 _CudaDeviceProperties(name='Tesla P100-PCIE-16GB', total_memory=16280MB) Fusing layers... Model Summary: 284 layers, 8.89222e+07 parameters, 0 gradients Scanning labels ../coco/labels/val2017.cache (4952 found, 0 missing, 48 empty, 0 duplicate, for 5000 images): 5000it [00:00, 17761.74it/s] Class Images Targets P R mAP@.5 mAP@.5:.95: 100% 157/157 [02:34<00:00, 1.02it/s] all 5e+03 3.63e+04 0.409 0.754 0.669 0.476 Speed: 23.6/1.6/25.2 ms inference/NMS/total per 640x640 image at batch-size 32 COCO mAP with pycocotools... saving detections_val2017__results.json... Average Precision (AP) @[ IoU=0.50:0.95 | area= all | maxDets=100 ] = 0.492 < ---------- baseline mAP Average Precision (AP) @[ IoU=0.50 | area= all | maxDets=100 ] = 0.676 Average Precision (AP) @[ IoU=0.75 | area= all | maxDets=100 ] = 0.534 Average Precision (AP) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.318 Average Precision (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.541 Average Precision (AP) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.633 Average Recall (AR) @[ IoU=0.50:0.95 | area= all | maxDets= 1 ] = 0.376 Average Recall (AR) @[ IoU=0.50:0.95 | area= all | maxDets= 10 ] = 0.616 Average Recall (AR) @[ IoU=0.50:0.95 | area= all | maxDets=100 ] = 0.670 Average Recall (AR) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.493 Average Recall (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.723 Average Recall (AR) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.812","title":"Test Normally"},{"location":"tutorials/model-ensembling/#ensemble-test","text":"Multiple pretraind models may be ensembled togethor at test and inference time by simply appending extra models to the --weights argument in any existing test.py or detect.py command. This example tests an ensemble of 2 models togethor: - YOLOv5x - YOLOv5l $ python test.py --weights yolov5x.pt yolov5l.pt --data coco.yaml --img 640 Output: Namespace(augment=False, batch_size=32, conf_thres=0.001, data='./data/coco.yaml', device='', img_size=640, iou_thres=0.65, save_json=True, save_txt=False, single_cls=False, task='val', verbose=False, weights=['yolov5x.pt', 'yolov5l.pt']) Using CUDA device0 _CudaDeviceProperties(name='Tesla P100-PCIE-16GB', total_memory=16280MB) Fusing layers... Model Summary: 284 layers, 8.89222e+07 parameters, 0 gradients # Model 1 Fusing layers... Model Summary: 236 layers, 4.77901e+07 parameters, 0 gradients # Model 2 Ensemble created with ['yolov5x.pt', 'yolov5l.pt'] # Ensemble Notice Scanning labels ../coco/labels/val2017.cache (4952 found, 0 missing, 48 empty, 0 duplicate, for 5000 images): 5000it [00:00, 17883.26it/s] Class Images Targets P R mAP@.5 mAP@.5:.95: 100% 157/157 [03:42<00:00, 1.42s/it] all 5e+03 3.63e+04 0.402 0.764 0.677 0.48 Speed: 37.5/1.4/38.9 ms inference/NMS/total per 640x640 image at batch-size 32 COCO mAP with pycocotools... saving detections_val2017__results.json... Average Precision (AP) @[ IoU=0.50:0.95 | area= all | maxDets=100 ] = 0.496 < ---------- improved mAP Average Precision (AP) @[ IoU=0.50 | area= all | maxDets=100 ] = 0.684 Average Precision (AP) @[ IoU=0.75 | area= all | maxDets=100 ] = 0.538 Average Precision (AP) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.323 Average Precision (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.548 Average Precision (AP) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.633 Average Recall (AR) @[ IoU=0.50:0.95 | area= all | maxDets= 1 ] = 0.377 Average Recall (AR) @[ IoU=0.50:0.95 | area= all | maxDets= 10 ] = 0.615 Average Recall (AR) @[ IoU=0.50:0.95 | area= all | maxDets=100 ] = 0.670 Average Recall (AR) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.495 Average Recall (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.723 Average Recall (AR) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.815","title":"Ensemble Test"},{"location":"tutorials/model-ensembling/#ensemble-inference","text":"Append extra models to the --weights argument to run ensemble inference: $ python detect.py --weights yolov5x.pt yolov5l.pt --img 640 --source ./inference/images/ Output: Namespace(agnostic_nms=False, augment=False, classes=None, conf_thres=0.25, device='', img_size=640, iou_thres=0.45, output='inference/output', save_txt=False, source='./inference/images/', update=False, view_img=False, weights=['yolov5x.pt', 'yolov5l.pt']) Using CUDA device0 _CudaDeviceProperties(name='Tesla P100-PCIE-16GB', total_memory=16280MB) Fusing layers... Model Summary: 284 layers, 8.89222e+07 parameters, 0 gradients # Model 1 Fusing layers... Model Summary: 236 layers, 4.77901e+07 parameters, 0 gradients # Model 2 Ensemble created with ['yolov5x.pt', 'yolov5l.pt'] # Ensemble Notice image 1/2 inference/images/bus.jpg: 640x512 4 persons, 1 bicycles, 1 buss, Done. (0.073s) image 2/2 inference/images/zidane.jpg: 384x640 3 persons, 3 ties, Done. (0.063s) Results saved to inference/output Done. (0.319s)","title":"Ensemble Inference"},{"location":"tutorials/model-ensembling/#environments","text":"YOLOv5 may be run in any of the following up-to-date verified environments (with all dependencies including CUDA / CUDNN , Python and PyTorch preinstalled): Google Colab and Kaggle notebooks with free GPU: Google Cloud Deep Learning VM. See GCP Quickstart Guide Amazon Deep Learning AMI. See AWS Quickstart Guide Docker Image . See Docker Quickstart Guide","title":"Environments"},{"location":"tutorials/model-ensembling/#status","text":"If this badge is green, all YOLOv5 GitHub Actions Continuous Integration (CI) tests are currently passing. CI tests verify correct operation of YOLOv5 training ( train.py ), testing ( test.py ), inference ( detect.py ) and export ( export.py ) on MacOS, Windows, and Ubuntu every 24 hours and on every commit.","title":"Status"},{"location":"tutorials/multi-gpu-training/","text":"\ud83d\udcda This guide explains how to properly use multiple GPUs to train a dataset with YOLOv5 \ud83d\ude80 on single or multiple machine(s). Before You Start Clone this repo and install requirements.txt dependencies, including Python>=3.8 and PyTorch>=1.7 . git clone https://github.com/ultralytics/yolov5 # clone repo cd yolov5 pip install -r requirements.txt Training Select a pretrained model to start training from. Here we select YOLOv5l , the smallest model available. See our README table for a full comparison of all models. We will train this model with Multi-GPU on the COCO dataset. Single GPU $ python train.py --batch-size 64 --data coco.yaml --weights yolov5s.pt --device 0 Multi-GPU DataParallel Mode (\u26a0\ufe0f not recommended) You can increase the device to use Multiple GPUs in DataParallel mode. $ python train.py --batch-size 64 --data coco.yaml --weights yolov5s.pt --device 0,1 This method is slow and barely speeds up training compared to using just 1 GPU. Multi-GPU DistributedDataParallel Mode (\u2705 recommended) You will have to pass python -m torch.distributed.launch --nproc_per_node , followed by the usual arguments. $ python -m torch.distributed.launch --nproc_per_node 2 train.py --batch-size 64 --data coco.yaml --weights yolov5s.pt --nproc_per_node specifies how many GPUs you would like to use. In the example above, it is 2. --batch-size is now the Total batch-size. It will be divided evenly to each GPU. In the example above, it is 64/2=32 per GPU. The code above will use GPUs 0... (N-1) . Use specific GPUs (click to expand) You can do so by simply passing `--device` followed by your specific GPUs. For example, in the code below, we will use GPUs `2,3`. $ python -m torch.distributed.launch --nproc_per_node 2 train.py --batch-size 64 --data coco.yaml --cfg yolov5s.yaml --weights '' --device 2,3 Use SyncBatchNorm (click to expand) [SyncBatchNorm](https://pytorch.org/docs/master/generated/torch.nn.SyncBatchNorm.html) could increase accuracy for multiple gpu training, however, it will slow down training by a significant factor. It is **only** available for Multiple GPU DistributedDataParallel training. It is best used when the batch-size on **each** GPU is small ( < = 8). To use SyncBatchNorm, simple pass `--sync-bn` to the command like below, $ python -m torch.distributed.launch --nproc_per_node 2 train.py --batch-size 64 --data coco.yaml --cfg yolov5s.yaml --weights '' --sync-bn Use Multiple machines (click to expand) This is **only** available for Multiple GPU DistributedDataParallel training. Before we continue, make sure the files on all machines are the same, dataset, codebase, etc. Afterwards, make sure the machines can communicate to each other. You will have to choose a master machine(the machine that the others will talk to). Note down its address(`master_addr`) and choose a port(`master_port`). I will use `master_addr = 192.168.1.1` and `master_port = 1234` for the example below. To use it, you can do as the following, # On master machine 0 $ python -m torch.distributed.launch --nproc_per_node G --nnodes N --node_rank 0 --master_addr \"192.168.1.1\" --master_port 1234 train.py --batch-size 64 --data coco.yaml --cfg yolov5s.yaml --weights '' # On machine R $ python -m torch.distributed.launch --nproc_per_node G --nnodes N --node_rank R --master_addr \"192.168.1.1\" --master_port 1234 train.py --batch-size 64 --data coco.yaml --cfg yolov5s.yaml --weights '' where `G` is number of GPU per machine, `N` is the number of machines, and `R` is the machine number from `0...(N-1)`. Let's say I have two machines with two GPUs each, it would be `G = 2` , `N = 2`, and `R = 1` for the above. Training will not start until all `N` machines are connected. Output will only be shown on master machine! Notes This does not work on Windows! batch-size must be a multiple of the number of GPUs! GPU 0 will take more memory than the other GPUs. (Edit: After 1.6 pytorch update, it may take even more memory.) If you get RuntimeError: Address already in use , it could be because you are running multiple trainings at a time. To fix this, simply use a different port number by adding --master_port like below, $ python -m torch.distributed.launch --master_port 1234 --nproc_per_node 2 ... Results Tested on COCO2017 dataset using V100s for 3 epochs with yolov5l and averaged. DistributedDataParallel mode. Command $ python train.py --batch-size 64 --data coco.yaml --cfg yolov5s.yaml --weights '' --device 0 $ python -m torch.distributed.launch --nproc_per_node 2 train.py --batch-size 64 --data coco.yaml --weights yolov5s.pt $ python -m torch.distributed.launch --nproc_per_node 4 train.py --batch-size 64 --data coco.yaml --weights yolov5s.pt $ python -m torch.distributed.launch --nproc_per_node 8 train.py --batch-size 64 --data coco.yaml --weights yolov5s.pt FAQ If an error occurs, please read the checklist below first! (It could save your time) Checklist (click to expand) Have you properly read this post? Have you tried to reclone the codebase? The code changes daily . Have you tried to search for your error? Someone may have already encountered it in this repo or in another and have the solution. Have you installed all the requirements listed on top (including the correct Python and Pytorch versions)? Have you tried in other environments listed in the \"Environments\" section below? Have you tried with another dataset like coco128 or coco2017? It will make it easier to find the root cause. If you went through all the above, feel free to raise an Issue by giving as much detail as possible following the template. Environments YOLOv5 may be run in any of the following up-to-date verified environments (with all dependencies including CUDA / CUDNN , Python and PyTorch preinstalled): Google Colab and Kaggle notebooks with free GPU: Google Cloud Deep Learning VM. See GCP Quickstart Guide Amazon Deep Learning AMI. See AWS Quickstart Guide Docker Image . See Docker Quickstart Guide Status If this badge is green, all YOLOv5 GitHub Actions Continuous Integration (CI) tests are currently passing. CI tests verify correct operation of YOLOv5 training ( train.py ), testing ( test.py ), inference ( detect.py ) and export ( export.py ) on MacOS, Windows, and Ubuntu every 24 hours and on every commit. Credits I would like to thank @MagicFrogSJTU, who did all the heavy lifting, and @glenn-jocher for guiding us along the way.","title":"Multi-GPU Training"},{"location":"tutorials/multi-gpu-training/#before-you-start","text":"Clone this repo and install requirements.txt dependencies, including Python>=3.8 and PyTorch>=1.7 . git clone https://github.com/ultralytics/yolov5 # clone repo cd yolov5 pip install -r requirements.txt","title":"Before You Start"},{"location":"tutorials/multi-gpu-training/#training","text":"Select a pretrained model to start training from. Here we select YOLOv5l , the smallest model available. See our README table for a full comparison of all models. We will train this model with Multi-GPU on the COCO dataset.","title":"Training"},{"location":"tutorials/multi-gpu-training/#single-gpu","text":"$ python train.py --batch-size 64 --data coco.yaml --weights yolov5s.pt --device 0","title":"Single GPU"},{"location":"tutorials/multi-gpu-training/#multi-gpu-dataparallel-mode-not-recommended","text":"You can increase the device to use Multiple GPUs in DataParallel mode. $ python train.py --batch-size 64 --data coco.yaml --weights yolov5s.pt --device 0,1 This method is slow and barely speeds up training compared to using just 1 GPU.","title":"Multi-GPU DataParallel Mode (\u26a0\ufe0f not recommended)"},{"location":"tutorials/multi-gpu-training/#multi-gpu-distributeddataparallel-mode-recommended","text":"You will have to pass python -m torch.distributed.launch --nproc_per_node , followed by the usual arguments. $ python -m torch.distributed.launch --nproc_per_node 2 train.py --batch-size 64 --data coco.yaml --weights yolov5s.pt --nproc_per_node specifies how many GPUs you would like to use. In the example above, it is 2. --batch-size is now the Total batch-size. It will be divided evenly to each GPU. In the example above, it is 64/2=32 per GPU. The code above will use GPUs 0... (N-1) . Use specific GPUs (click to expand) You can do so by simply passing `--device` followed by your specific GPUs. For example, in the code below, we will use GPUs `2,3`. $ python -m torch.distributed.launch --nproc_per_node 2 train.py --batch-size 64 --data coco.yaml --cfg yolov5s.yaml --weights '' --device 2,3 Use SyncBatchNorm (click to expand) [SyncBatchNorm](https://pytorch.org/docs/master/generated/torch.nn.SyncBatchNorm.html) could increase accuracy for multiple gpu training, however, it will slow down training by a significant factor. It is **only** available for Multiple GPU DistributedDataParallel training. It is best used when the batch-size on **each** GPU is small ( < = 8). To use SyncBatchNorm, simple pass `--sync-bn` to the command like below, $ python -m torch.distributed.launch --nproc_per_node 2 train.py --batch-size 64 --data coco.yaml --cfg yolov5s.yaml --weights '' --sync-bn Use Multiple machines (click to expand) This is **only** available for Multiple GPU DistributedDataParallel training. Before we continue, make sure the files on all machines are the same, dataset, codebase, etc. Afterwards, make sure the machines can communicate to each other. You will have to choose a master machine(the machine that the others will talk to). Note down its address(`master_addr`) and choose a port(`master_port`). I will use `master_addr = 192.168.1.1` and `master_port = 1234` for the example below. To use it, you can do as the following, # On master machine 0 $ python -m torch.distributed.launch --nproc_per_node G --nnodes N --node_rank 0 --master_addr \"192.168.1.1\" --master_port 1234 train.py --batch-size 64 --data coco.yaml --cfg yolov5s.yaml --weights '' # On machine R $ python -m torch.distributed.launch --nproc_per_node G --nnodes N --node_rank R --master_addr \"192.168.1.1\" --master_port 1234 train.py --batch-size 64 --data coco.yaml --cfg yolov5s.yaml --weights '' where `G` is number of GPU per machine, `N` is the number of machines, and `R` is the machine number from `0...(N-1)`. Let's say I have two machines with two GPUs each, it would be `G = 2` , `N = 2`, and `R = 1` for the above. Training will not start until all `N` machines are connected. Output will only be shown on master machine!","title":"Multi-GPU DistributedDataParallel Mode (\u2705 recommended)"},{"location":"tutorials/multi-gpu-training/#notes","text":"This does not work on Windows! batch-size must be a multiple of the number of GPUs! GPU 0 will take more memory than the other GPUs. (Edit: After 1.6 pytorch update, it may take even more memory.) If you get RuntimeError: Address already in use , it could be because you are running multiple trainings at a time. To fix this, simply use a different port number by adding --master_port like below, $ python -m torch.distributed.launch --master_port 1234 --nproc_per_node 2 ...","title":"Notes"},{"location":"tutorials/multi-gpu-training/#results","text":"Tested on COCO2017 dataset using V100s for 3 epochs with yolov5l and averaged. DistributedDataParallel mode. Command $ python train.py --batch-size 64 --data coco.yaml --cfg yolov5s.yaml --weights '' --device 0 $ python -m torch.distributed.launch --nproc_per_node 2 train.py --batch-size 64 --data coco.yaml --weights yolov5s.pt $ python -m torch.distributed.launch --nproc_per_node 4 train.py --batch-size 64 --data coco.yaml --weights yolov5s.pt $ python -m torch.distributed.launch --nproc_per_node 8 train.py --batch-size 64 --data coco.yaml --weights yolov5s.pt","title":"Results"},{"location":"tutorials/multi-gpu-training/#faq","text":"If an error occurs, please read the checklist below first! (It could save your time) Checklist (click to expand) Have you properly read this post? Have you tried to reclone the codebase? The code changes daily . Have you tried to search for your error? Someone may have already encountered it in this repo or in another and have the solution. Have you installed all the requirements listed on top (including the correct Python and Pytorch versions)? Have you tried in other environments listed in the \"Environments\" section below? Have you tried with another dataset like coco128 or coco2017? It will make it easier to find the root cause. If you went through all the above, feel free to raise an Issue by giving as much detail as possible following the template.","title":"FAQ"},{"location":"tutorials/multi-gpu-training/#environments","text":"YOLOv5 may be run in any of the following up-to-date verified environments (with all dependencies including CUDA / CUDNN , Python and PyTorch preinstalled): Google Colab and Kaggle notebooks with free GPU: Google Cloud Deep Learning VM. See GCP Quickstart Guide Amazon Deep Learning AMI. See AWS Quickstart Guide Docker Image . See Docker Quickstart Guide","title":"Environments"},{"location":"tutorials/multi-gpu-training/#status","text":"If this badge is green, all YOLOv5 GitHub Actions Continuous Integration (CI) tests are currently passing. CI tests verify correct operation of YOLOv5 training ( train.py ), testing ( test.py ), inference ( detect.py ) and export ( export.py ) on MacOS, Windows, and Ubuntu every 24 hours and on every commit.","title":"Status"},{"location":"tutorials/multi-gpu-training/#credits","text":"I would like to thank @MagicFrogSJTU, who did all the heavy lifting, and @glenn-jocher for guiding us along the way.","title":"Credits"},{"location":"tutorials/pruning-sparsity/","text":"\ud83d\udcda This guide explains how to apply pruning to YOLOv5 \ud83d\ude80 models. Before You Start Clone YOLOv5 repo and install requirements.txt dependencies, including Python>=3.8 and PyTorch>=1.7 . $ git clone https://github.com/ultralytics/yolov5 # clone repo $ cd yolov5 $ pip install -r requirements.txt # install Test YOLOv5x on COCO (default) This command tests YOLOv5x on COCO val2017 at image size 640 pixels to establish a nominal baseline. yolov5x.pt is the largest and most accurate model available. Other options are yolov5s.pt , yolov5m.pt and yolov5l.pt , or you own checkpoint from training a custom dataset ./weights/best.pt . For details on all available models please see our README table . $ python test.py --weights yolov5x.pt --data coco.yaml --img 640 --iou 0.65 Default output: YOLOv5 \ud83d\ude80 v4.0-174-g9c803f2 torch 1.8.0+cu101 CUDA:0 (Tesla V100-SXM2-16GB, 16160.5MB) Fusing layers... Model Summary: 476 layers, 87730285 parameters, 0 gradients, 218.8 GFLOPS val: Scanning '../coco/val2017.cache' images and labels... 4952 found, 48 missing, 0 empty, 0 corrupted: 100% 5000/5000 [00:00<00:00, 50901747.57it/s] Class Images Labels P R mAP@.5 mAP@.5:.95: 100% 157/157 [01:20<00:00, 1.95it/s] all 5000 36335 0.749 0.619 0.68 0.486 Speed: 5.2/1.6/6.8 ms inference/NMS/total per 640x640 image at batch-size 32 < -------- speed Evaluating pycocotools mAP... saving runs/test/exp/yolov5x_predictions.json... Average Precision (AP) @[ IoU=0.50:0.95 | area= all | maxDets=100 ] = 0.501 < -------- mAP Average Precision (AP) @[ IoU=0.50 | area= all | maxDets=100 ] = 0.687 Average Precision (AP) @[ IoU=0.75 | area= all | maxDets=100 ] = 0.544 Average Precision (AP) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.338 Average Precision (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.548 Average Precision (AP) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.637 Average Recall (AR) @[ IoU=0.50:0.95 | area= all | maxDets= 1 ] = 0.378 < -------- mAR Average Recall (AR) @[ IoU=0.50:0.95 | area= all | maxDets= 10 ] = 0.628 Average Recall (AR) @[ IoU=0.50:0.95 | area= all | maxDets=100 ] = 0.680 Average Recall (AR) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.520 Average Recall (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.729 Average Recall (AR) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.826 Results saved to runs/test/exp Test YOLOv5x on COCO (0.30 sparsity) We repeat the above test with a pruned model by using the torch_utils.prune() command. We update test.py to prune YOLOv5x to 0.3 sparsity: 30% pruned output: YOLOv5 \ud83d\ude80 v4.0-174-g9c803f2 torch 1.8.0+cu101 CUDA:0 (Tesla V100-SXM2-16GB, 16160.5MB) Fusing layers... Model Summary: 476 layers, 87730285 parameters, 0 gradients, 218.8 GFLOPS Pruning model... 0.3 global sparsity val: Scanning '../coco/val2017.cache' images and labels... 4952 found, 48 missing, 0 empty, 0 corrupted: 100% 5000/5000 [00:00<00:00, 50901747.57it/s] Class Images Labels P R mAP@.5 mAP@.5:.95: 100% 157/157 [01:17<00:00, 2.01it/s] all 5000 36335 0.742 0.595 0.66 0.46 Speed: 5.2/1.5/6.7 ms inference/NMS/total per 640x640 image at batch-size 32 < -------- speed Evaluating pycocotools mAP... saving runs/test/exp/yolov5x_predictions.json... Average Precision (AP) @[ IoU=0.50:0.95 | area= all | maxDets=100 ] = 0.477 < -------- mAP Average Precision (AP) @[ IoU=0.50 | area= all | maxDets=100 ] = 0.667 Average Precision (AP) @[ IoU=0.75 | area= all | maxDets=100 ] = 0.525 Average Precision (AP) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.313 Average Precision (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.530 Average Precision (AP) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.619 Average Recall (AR) @[ IoU=0.50:0.95 | area= all | maxDets= 1 ] = 0.366 < -------- mAR Average Recall (AR) @[ IoU=0.50:0.95 | area= all | maxDets= 10 ] = 0.603 Average Recall (AR) @[ IoU=0.50:0.95 | area= all | maxDets=100 ] = 0.654 Average Recall (AR) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.475 Average Recall (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.709 Average Recall (AR) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.794 Results saved to runs/test/exp In the results we can observe that we have achieved a sparsity of 30% in our model after pruning, which means that 30% of the model's weight parameters in nn.Conv2d layers are equal to 0. Inference time is essentially unchanged , while the model's AP and AR scores a slightly reduced . Environments YOLOv5 may be run in any of the following up-to-date verified environments (with all dependencies including CUDA / CUDNN , Python and PyTorch preinstalled): Google Colab and Kaggle notebooks with free GPU: Google Cloud Deep Learning VM. See GCP Quickstart Guide Amazon Deep Learning AMI. See AWS Quickstart Guide Docker Image . See Docker Quickstart Guide Status If this badge is green, all YOLOv5 GitHub Actions Continuous Integration (CI) tests are currently passing. CI tests verify correct operation of YOLOv5 training ( train.py ), testing ( test.py ), inference ( detect.py ) and export ( export.py ) on MacOS, Windows, and Ubuntu every 24 hours and on every commit.","title":"Model Pruning/Sparsity"},{"location":"tutorials/pruning-sparsity/#before-you-start","text":"Clone YOLOv5 repo and install requirements.txt dependencies, including Python>=3.8 and PyTorch>=1.7 . $ git clone https://github.com/ultralytics/yolov5 # clone repo $ cd yolov5 $ pip install -r requirements.txt # install","title":"Before You Start"},{"location":"tutorials/pruning-sparsity/#test-yolov5x-on-coco-default","text":"This command tests YOLOv5x on COCO val2017 at image size 640 pixels to establish a nominal baseline. yolov5x.pt is the largest and most accurate model available. Other options are yolov5s.pt , yolov5m.pt and yolov5l.pt , or you own checkpoint from training a custom dataset ./weights/best.pt . For details on all available models please see our README table . $ python test.py --weights yolov5x.pt --data coco.yaml --img 640 --iou 0.65 Default output: YOLOv5 \ud83d\ude80 v4.0-174-g9c803f2 torch 1.8.0+cu101 CUDA:0 (Tesla V100-SXM2-16GB, 16160.5MB) Fusing layers... Model Summary: 476 layers, 87730285 parameters, 0 gradients, 218.8 GFLOPS val: Scanning '../coco/val2017.cache' images and labels... 4952 found, 48 missing, 0 empty, 0 corrupted: 100% 5000/5000 [00:00<00:00, 50901747.57it/s] Class Images Labels P R mAP@.5 mAP@.5:.95: 100% 157/157 [01:20<00:00, 1.95it/s] all 5000 36335 0.749 0.619 0.68 0.486 Speed: 5.2/1.6/6.8 ms inference/NMS/total per 640x640 image at batch-size 32 < -------- speed Evaluating pycocotools mAP... saving runs/test/exp/yolov5x_predictions.json... Average Precision (AP) @[ IoU=0.50:0.95 | area= all | maxDets=100 ] = 0.501 < -------- mAP Average Precision (AP) @[ IoU=0.50 | area= all | maxDets=100 ] = 0.687 Average Precision (AP) @[ IoU=0.75 | area= all | maxDets=100 ] = 0.544 Average Precision (AP) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.338 Average Precision (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.548 Average Precision (AP) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.637 Average Recall (AR) @[ IoU=0.50:0.95 | area= all | maxDets= 1 ] = 0.378 < -------- mAR Average Recall (AR) @[ IoU=0.50:0.95 | area= all | maxDets= 10 ] = 0.628 Average Recall (AR) @[ IoU=0.50:0.95 | area= all | maxDets=100 ] = 0.680 Average Recall (AR) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.520 Average Recall (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.729 Average Recall (AR) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.826 Results saved to runs/test/exp","title":"Test YOLOv5x on COCO (default)"},{"location":"tutorials/pruning-sparsity/#test-yolov5x-on-coco-030-sparsity","text":"We repeat the above test with a pruned model by using the torch_utils.prune() command. We update test.py to prune YOLOv5x to 0.3 sparsity: 30% pruned output: YOLOv5 \ud83d\ude80 v4.0-174-g9c803f2 torch 1.8.0+cu101 CUDA:0 (Tesla V100-SXM2-16GB, 16160.5MB) Fusing layers... Model Summary: 476 layers, 87730285 parameters, 0 gradients, 218.8 GFLOPS Pruning model... 0.3 global sparsity val: Scanning '../coco/val2017.cache' images and labels... 4952 found, 48 missing, 0 empty, 0 corrupted: 100% 5000/5000 [00:00<00:00, 50901747.57it/s] Class Images Labels P R mAP@.5 mAP@.5:.95: 100% 157/157 [01:17<00:00, 2.01it/s] all 5000 36335 0.742 0.595 0.66 0.46 Speed: 5.2/1.5/6.7 ms inference/NMS/total per 640x640 image at batch-size 32 < -------- speed Evaluating pycocotools mAP... saving runs/test/exp/yolov5x_predictions.json... Average Precision (AP) @[ IoU=0.50:0.95 | area= all | maxDets=100 ] = 0.477 < -------- mAP Average Precision (AP) @[ IoU=0.50 | area= all | maxDets=100 ] = 0.667 Average Precision (AP) @[ IoU=0.75 | area= all | maxDets=100 ] = 0.525 Average Precision (AP) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.313 Average Precision (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.530 Average Precision (AP) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.619 Average Recall (AR) @[ IoU=0.50:0.95 | area= all | maxDets= 1 ] = 0.366 < -------- mAR Average Recall (AR) @[ IoU=0.50:0.95 | area= all | maxDets= 10 ] = 0.603 Average Recall (AR) @[ IoU=0.50:0.95 | area= all | maxDets=100 ] = 0.654 Average Recall (AR) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.475 Average Recall (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.709 Average Recall (AR) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.794 Results saved to runs/test/exp In the results we can observe that we have achieved a sparsity of 30% in our model after pruning, which means that 30% of the model's weight parameters in nn.Conv2d layers are equal to 0. Inference time is essentially unchanged , while the model's AP and AR scores a slightly reduced .","title":"Test YOLOv5x on COCO (0.30 sparsity)"},{"location":"tutorials/pruning-sparsity/#environments","text":"YOLOv5 may be run in any of the following up-to-date verified environments (with all dependencies including CUDA / CUDNN , Python and PyTorch preinstalled): Google Colab and Kaggle notebooks with free GPU: Google Cloud Deep Learning VM. See GCP Quickstart Guide Amazon Deep Learning AMI. See AWS Quickstart Guide Docker Image . See Docker Quickstart Guide","title":"Environments"},{"location":"tutorials/pruning-sparsity/#status","text":"If this badge is green, all YOLOv5 GitHub Actions Continuous Integration (CI) tests are currently passing. CI tests verify correct operation of YOLOv5 training ( train.py ), testing ( test.py ), inference ( detect.py ) and export ( export.py ) on MacOS, Windows, and Ubuntu every 24 hours and on every commit.","title":"Status"},{"location":"tutorials/pytorch-hub/","text":"\ud83d\udcda This guide explains how to load YOLOv5 \ud83d\ude80 from PyTorch Hub https://pytorch.org/hub/ultralytics_yolov5 Before You Start Start from a Python>=3.8 environment with PyTorch>=1.7 installed, as well as pyyaml>=5.3 for reading YOLOv5 configuration files. To install PyTorch see https://pytorch.org/get-started/locally . To install YOLOv5 requirements : $ pip install -r https://raw.githubusercontent.com/ultralytics/yolov5/master/requirements.txt Cloning the https://github.com/ultralytics/yolov5 repository is not required \ud83d\ude03. Load YOLOv5 with PyTorch Hub Simple Example This example loads a pretrained YOLOv5s model from PyTorch Hub as model and passes an image for inference. 'yolov5s' is the lightest and fastest YOLOv5 model. For details on all available models please see the README . import torch # Model model = torch.hub.load('ultralytics/yolov5', 'yolov5s') # Image img = 'https://ultralytics.com/images/zidane.jpg' # Inference results = model(img) Detailed Example This example shows batched inference with PIL and OpenCV image sources. results can be printed to console, saved to runs/hub , showed to screen on supported environments, and returned as tensors or pandas dataframes. import cv2 import torch from PIL import Image # Model model = torch.hub.load('ultralytics/yolov5', 'yolov5s') # Images for f in ['zidane.jpg', 'bus.jpg']: torch.hub.download_url_to_file('https://ultralytics.com/images/' + f, f) # download 2 images img1 = Image.open('zidane.jpg') # PIL image img2 = cv2.imread('bus.jpg')[:, :, ::-1] # OpenCV image (BGR to RGB) imgs = [img1, img2] # batch of images # Inference results = model(imgs, size=640) # includes NMS # Results results.print() results.save() # or .show() results.xyxy[0] # img1 predictions (tensor) results.pandas().xyxy[0] # img1 predictions (pandas) # xmin ymin xmax ymax confidence class name # 0 749.50 43.50 1148.0 704.5 0.874023 0 person # 1 433.50 433.50 517.5 714.5 0.687988 27 tie # 2 114.75 195.75 1095.0 708.0 0.624512 0 person # 3 986.00 304.00 1028.0 420.0 0.286865 27 tie For all inference options see YOLOv5 autoShape() forward method: https://github.com/ultralytics/yolov5/blob/3551b072b366989b82b3777c63ea485a99e0bf90/models/common.py#L182-L191 Inference Settings Inference settings such as confidence threshold , NMS IoU threshold , and classes filter are model attributes, and can be modified by: model.conf = 0.25 # confidence threshold (0-1) model.iou = 0.45 # NMS IoU threshold (0-1) model.classes = None # (optional list) filter by class, i.e. = [0, 15, 16] for persons, cats and dogs results = model(imgs, size=320) # custom inference size Input Channels To load a pretrained YOLOv5s model with 4 input channels rather than the default 3: model = torch.hub.load('ultralytics/yolov5', 'yolov5s', channels=4) In this case the model will be composed of pretrained weights except for the very first input layer, which is no longer the same shape as the pretrained input layer. The input layer will remain initialized by random weights. Number of Classes To load a pretrained YOLOv5s model with 10 output classes rather than the default 80: model = torch.hub.load('ultralytics/yolov5', 'yolov5s', classes=10) In this case the model will be composed of pretrained weights except for the output layers, which are no longer the same shape as the pretrained output layers. The output layers will remain initialized by random weights. Force Reload If you run into problems with the above steps, setting force_reload=True may help by discarding the existing cache and force a fresh download of the latest YOLOv5 version from PyTorch Hub. model = torch.hub.load('ultralytics/yolov5', 'yolov5s', force_reload=True) # force reload Training To load a YOLOv5 model for training rather than inference, set autoshape=False . To load a model with randomly initialized weights (to train from scratch) use pretrained=False . model = torch.hub.load('ultralytics/yolov5', 'yolov5s', autoshape=False) # load pretrained model = torch.hub.load('ultralytics/yolov5', 'yolov5s', autoshape=False, pretrained=False) # load scratch Base64 Results For use with API services. See https://github.com/ultralytics/yolov5/pull/2291 and Flask REST API example for details. results = model(imgs) # inference results.imgs # array of original images (as np array) passed to model for inference results.render() # updates results.imgs with boxes and labels for img in results.imgs: buffered = BytesIO() img_base64 = Image.fromarray(img) img_base64.save(buffered, format=\"JPEG\") print(base64.b64encode(buffered.getvalue()).decode('utf-8')) # base64 encoded image with results JSON Results Results can be returned in JSON format once converted to .pandas() dataframes using the .to_json() method. The JSON format can be modified using the orient argument. See pandas .to_json() documentation for details. results = model(imgs) # inference results.pandas().xyxy[0].to_json(orient=\"records\") # JSON img1 predictions JSON Output (click to expand) ```json [ {\"xmin\":749.5,\"ymin\":43.5,\"xmax\":1148.0,\"ymax\":704.5,\"confidence\":0.8740234375,\"class\":0,\"name\":\"person\"}, {\"xmin\":433.5,\"ymin\":433.5,\"xmax\":517.5,\"ymax\":714.5,\"confidence\":0.6879882812,\"class\":27,\"name\":\"tie\"}, {\"xmin\":115.25,\"ymin\":195.75,\"xmax\":1096.0,\"ymax\":708.0,\"confidence\":0.6254882812,\"class\":0,\"name\":\"person\"}, {\"xmin\":986.0,\"ymin\":304.0,\"xmax\":1028.0,\"ymax\":420.0,\"confidence\":0.2873535156,\"class\":27,\"name\":\"tie\"} ] ``` Custom Models This example loads a custom 20-class VOC -trained YOLOv5s model 'best.pt' with PyTorch Hub. model = torch.hub.load('ultralytics/yolov5', 'custom', path='path/to/best.pt') # default model = torch.hub.load('path/to/yolov5', 'custom', path='path/to/best.pt', source='local') # local repo Environments YOLOv5 may be run in any of the following up-to-date verified environments (with all dependencies including CUDA / CUDNN , Python and PyTorch preinstalled): Google Colab and Kaggle notebooks with free GPU: Google Cloud Deep Learning VM. See GCP Quickstart Guide Amazon Deep Learning AMI. See AWS Quickstart Guide Docker Image . See Docker Quickstart Guide Status If this badge is green, all YOLOv5 GitHub Actions Continuous Integration (CI) tests are currently passing. CI tests verify correct operation of YOLOv5 training ( train.py ), testing ( test.py ), inference ( detect.py ) and export ( export.py ) on MacOS, Windows, and Ubuntu every 24 hours and on every commit.","title":"PyTorch Hub"},{"location":"tutorials/pytorch-hub/#before-you-start","text":"Start from a Python>=3.8 environment with PyTorch>=1.7 installed, as well as pyyaml>=5.3 for reading YOLOv5 configuration files. To install PyTorch see https://pytorch.org/get-started/locally . To install YOLOv5 requirements : $ pip install -r https://raw.githubusercontent.com/ultralytics/yolov5/master/requirements.txt Cloning the https://github.com/ultralytics/yolov5 repository is not required \ud83d\ude03.","title":"Before You Start"},{"location":"tutorials/pytorch-hub/#load-yolov5-with-pytorch-hub","text":"","title":"Load YOLOv5 with PyTorch Hub"},{"location":"tutorials/pytorch-hub/#simple-example","text":"This example loads a pretrained YOLOv5s model from PyTorch Hub as model and passes an image for inference. 'yolov5s' is the lightest and fastest YOLOv5 model. For details on all available models please see the README . import torch # Model model = torch.hub.load('ultralytics/yolov5', 'yolov5s') # Image img = 'https://ultralytics.com/images/zidane.jpg' # Inference results = model(img)","title":"Simple Example"},{"location":"tutorials/pytorch-hub/#detailed-example","text":"This example shows batched inference with PIL and OpenCV image sources. results can be printed to console, saved to runs/hub , showed to screen on supported environments, and returned as tensors or pandas dataframes. import cv2 import torch from PIL import Image # Model model = torch.hub.load('ultralytics/yolov5', 'yolov5s') # Images for f in ['zidane.jpg', 'bus.jpg']: torch.hub.download_url_to_file('https://ultralytics.com/images/' + f, f) # download 2 images img1 = Image.open('zidane.jpg') # PIL image img2 = cv2.imread('bus.jpg')[:, :, ::-1] # OpenCV image (BGR to RGB) imgs = [img1, img2] # batch of images # Inference results = model(imgs, size=640) # includes NMS # Results results.print() results.save() # or .show() results.xyxy[0] # img1 predictions (tensor) results.pandas().xyxy[0] # img1 predictions (pandas) # xmin ymin xmax ymax confidence class name # 0 749.50 43.50 1148.0 704.5 0.874023 0 person # 1 433.50 433.50 517.5 714.5 0.687988 27 tie # 2 114.75 195.75 1095.0 708.0 0.624512 0 person # 3 986.00 304.00 1028.0 420.0 0.286865 27 tie For all inference options see YOLOv5 autoShape() forward method: https://github.com/ultralytics/yolov5/blob/3551b072b366989b82b3777c63ea485a99e0bf90/models/common.py#L182-L191","title":"Detailed Example"},{"location":"tutorials/pytorch-hub/#inference-settings","text":"Inference settings such as confidence threshold , NMS IoU threshold , and classes filter are model attributes, and can be modified by: model.conf = 0.25 # confidence threshold (0-1) model.iou = 0.45 # NMS IoU threshold (0-1) model.classes = None # (optional list) filter by class, i.e. = [0, 15, 16] for persons, cats and dogs results = model(imgs, size=320) # custom inference size","title":"Inference Settings"},{"location":"tutorials/pytorch-hub/#input-channels","text":"To load a pretrained YOLOv5s model with 4 input channels rather than the default 3: model = torch.hub.load('ultralytics/yolov5', 'yolov5s', channels=4) In this case the model will be composed of pretrained weights except for the very first input layer, which is no longer the same shape as the pretrained input layer. The input layer will remain initialized by random weights.","title":"Input Channels"},{"location":"tutorials/pytorch-hub/#number-of-classes","text":"To load a pretrained YOLOv5s model with 10 output classes rather than the default 80: model = torch.hub.load('ultralytics/yolov5', 'yolov5s', classes=10) In this case the model will be composed of pretrained weights except for the output layers, which are no longer the same shape as the pretrained output layers. The output layers will remain initialized by random weights.","title":"Number of Classes"},{"location":"tutorials/pytorch-hub/#force-reload","text":"If you run into problems with the above steps, setting force_reload=True may help by discarding the existing cache and force a fresh download of the latest YOLOv5 version from PyTorch Hub. model = torch.hub.load('ultralytics/yolov5', 'yolov5s', force_reload=True) # force reload","title":"Force Reload"},{"location":"tutorials/pytorch-hub/#training","text":"To load a YOLOv5 model for training rather than inference, set autoshape=False . To load a model with randomly initialized weights (to train from scratch) use pretrained=False . model = torch.hub.load('ultralytics/yolov5', 'yolov5s', autoshape=False) # load pretrained model = torch.hub.load('ultralytics/yolov5', 'yolov5s', autoshape=False, pretrained=False) # load scratch","title":"Training"},{"location":"tutorials/pytorch-hub/#base64-results","text":"For use with API services. See https://github.com/ultralytics/yolov5/pull/2291 and Flask REST API example for details. results = model(imgs) # inference results.imgs # array of original images (as np array) passed to model for inference results.render() # updates results.imgs with boxes and labels for img in results.imgs: buffered = BytesIO() img_base64 = Image.fromarray(img) img_base64.save(buffered, format=\"JPEG\") print(base64.b64encode(buffered.getvalue()).decode('utf-8')) # base64 encoded image with results","title":"Base64 Results"},{"location":"tutorials/pytorch-hub/#json-results","text":"Results can be returned in JSON format once converted to .pandas() dataframes using the .to_json() method. The JSON format can be modified using the orient argument. See pandas .to_json() documentation for details. results = model(imgs) # inference results.pandas().xyxy[0].to_json(orient=\"records\") # JSON img1 predictions JSON Output (click to expand) ```json [ {\"xmin\":749.5,\"ymin\":43.5,\"xmax\":1148.0,\"ymax\":704.5,\"confidence\":0.8740234375,\"class\":0,\"name\":\"person\"}, {\"xmin\":433.5,\"ymin\":433.5,\"xmax\":517.5,\"ymax\":714.5,\"confidence\":0.6879882812,\"class\":27,\"name\":\"tie\"}, {\"xmin\":115.25,\"ymin\":195.75,\"xmax\":1096.0,\"ymax\":708.0,\"confidence\":0.6254882812,\"class\":0,\"name\":\"person\"}, {\"xmin\":986.0,\"ymin\":304.0,\"xmax\":1028.0,\"ymax\":420.0,\"confidence\":0.2873535156,\"class\":27,\"name\":\"tie\"} ] ```","title":"JSON Results"},{"location":"tutorials/pytorch-hub/#custom-models","text":"This example loads a custom 20-class VOC -trained YOLOv5s model 'best.pt' with PyTorch Hub. model = torch.hub.load('ultralytics/yolov5', 'custom', path='path/to/best.pt') # default model = torch.hub.load('path/to/yolov5', 'custom', path='path/to/best.pt', source='local') # local repo","title":"Custom Models"},{"location":"tutorials/pytorch-hub/#environments","text":"YOLOv5 may be run in any of the following up-to-date verified environments (with all dependencies including CUDA / CUDNN , Python and PyTorch preinstalled): Google Colab and Kaggle notebooks with free GPU: Google Cloud Deep Learning VM. See GCP Quickstart Guide Amazon Deep Learning AMI. See AWS Quickstart Guide Docker Image . See Docker Quickstart Guide","title":"Environments"},{"location":"tutorials/pytorch-hub/#status","text":"If this badge is green, all YOLOv5 GitHub Actions Continuous Integration (CI) tests are currently passing. CI tests verify correct operation of YOLOv5 training ( train.py ), testing ( test.py ), inference ( detect.py ) and export ( export.py ) on MacOS, Windows, and Ubuntu every 24 hours and on every commit.","title":"Status"},{"location":"tutorials/supervisely-ecosystem/","text":"\ud83d\udcda This guide explains how to use Supervisely with YOLOv5 \ud83d\ude80. Table of Contents About Supervisely Prerequisites YOLOv5 Apps Collection For developers Contact & Questions & Suggestions \ud83d\udd25 About Supervisely You can think of Supervisely as an Operating System available via Web Browser to help you solve Computer Vision tasks. The idea is to unify all the relevant tools that may be needed to make the development process as smooth and fast as possible. More concretely, Supervisely includes the following functionality: - Data labeling for images, videos, 3D point cloud and volumetric medical images (dicom) - Data visualization and quality control - State-Of-The-Art Deep Learning models for segmentation, detection, classification and other tasks - Interactive tools for model performance analysis - Specialized Deep Learning models to speed up data labeling (aka AI-assisted labeling) - Synthetic data generation tools - Instruments to make it easier to collaborate for data scientists, data labelers, domain experts and software engineers One challenge is to make it possible for everyone to train and apply SOTA Deep Learning models directly from the Web Browser. To address it, we introduce an open sourced Supervisely Agent. All you need to do is to execute a single command on your machine with the GPU that installs the Agent. After that, you keep working in the browser and all the GPU related computations will be performed on the connected machine(s). Prerequisites You should connect computer with GPU to your Supervisely account. If you already have Supervisely Agent running on your computer, you can skip this step. Several tools have to be installed on your computer: Nvidia drives + CUDA Toolkit Docker NVIDIA Container Toolkit Once your computer is ready just add agent to your team and execute automatically generated running command in terminal. Watch how-to video: \ud83c\udf89 YOLO v5 Apps Collection YOLOv5 is one of the best available detectors. And we are proud to announce its full integrtion into Supervisely Ecosystem . To learn more about how to use every app, please go to app's readme page (links are provided). Just add the apps to your team to start using them. YOLOv5 Collection consists of the following apps: Train YOLOv5 - start training on your custom data. Just run app from the context menu of your project, choose classes of interest, train/val splits, configure training metaparameters and augmentations, and monitor training metrics in realtime. App automatically converts all labels to rectangles. All training artifacts including model weights will be saved to Team Files and can be easily downloaded. Serve YOLOv5 - serve model as Rest API service. You can run pretrained model, use custom model weights trained in Supervisely as well as weights trained outside (just upload weights file to Team Files). Thus other apps from Ecosystem can get predictions from the deployed model. Also developers can send inference requiests in a few lines of python code. Apply NN to images project - app allows to play with different inference options and visualize predictions in real time. Once you choose inference settings you can apply model to all images in your project to visually analise predictions and perform automatic data pre-labeling. NN Image Labeling - integrate any deployd NN to Supervisely Image Labeling UI. Configure inference settings and model output classes. Press Apply button (or use hotkey) and detections with their confidences will immediately appear on the image. Convert Supervisely to YOLO v5 format - export labeled images project in yolov5 compatible format. Convert YOLO v5 to Supervisely format - import images and yolov5 annotatons to Supervisely. For Developers you can use sources of Serve YOLOv5 app as example of how to prepare weights, initialize model and apply it to a folder with images (or to images URLs) This apps collection is based on the original YOLOv5 release v5.0 . Once a next official release is available, all apps will be synchronized with it and also released with the new versions. Before running any app you can choose what version to use. Also Supervisely Team will pull updates from original master branch from time to time. Contact & Questions & Suggestions for technical support please leave issues, questions or suggestions to original YOLOv5 repo with the prefix [Supervisely] . Our team will try to help. also we can chat in slack channel","title":"Supervisely Ecosystem \ud83c\udd95"},{"location":"tutorials/supervisely-ecosystem/#table-of-contents","text":"About Supervisely Prerequisites YOLOv5 Apps Collection For developers Contact & Questions & Suggestions","title":"Table of Contents"},{"location":"tutorials/supervisely-ecosystem/#about-supervisely","text":"You can think of Supervisely as an Operating System available via Web Browser to help you solve Computer Vision tasks. The idea is to unify all the relevant tools that may be needed to make the development process as smooth and fast as possible. More concretely, Supervisely includes the following functionality: - Data labeling for images, videos, 3D point cloud and volumetric medical images (dicom) - Data visualization and quality control - State-Of-The-Art Deep Learning models for segmentation, detection, classification and other tasks - Interactive tools for model performance analysis - Specialized Deep Learning models to speed up data labeling (aka AI-assisted labeling) - Synthetic data generation tools - Instruments to make it easier to collaborate for data scientists, data labelers, domain experts and software engineers One challenge is to make it possible for everyone to train and apply SOTA Deep Learning models directly from the Web Browser. To address it, we introduce an open sourced Supervisely Agent. All you need to do is to execute a single command on your machine with the GPU that installs the Agent. After that, you keep working in the browser and all the GPU related computations will be performed on the connected machine(s).","title":"\ud83d\udd25 About Supervisely"},{"location":"tutorials/supervisely-ecosystem/#prerequisites","text":"You should connect computer with GPU to your Supervisely account. If you already have Supervisely Agent running on your computer, you can skip this step. Several tools have to be installed on your computer: Nvidia drives + CUDA Toolkit Docker NVIDIA Container Toolkit Once your computer is ready just add agent to your team and execute automatically generated running command in terminal. Watch how-to video:","title":"Prerequisites"},{"location":"tutorials/supervisely-ecosystem/#yolo-v5-apps-collection","text":"YOLOv5 is one of the best available detectors. And we are proud to announce its full integrtion into Supervisely Ecosystem . To learn more about how to use every app, please go to app's readme page (links are provided). Just add the apps to your team to start using them. YOLOv5 Collection consists of the following apps: Train YOLOv5 - start training on your custom data. Just run app from the context menu of your project, choose classes of interest, train/val splits, configure training metaparameters and augmentations, and monitor training metrics in realtime. App automatically converts all labels to rectangles. All training artifacts including model weights will be saved to Team Files and can be easily downloaded. Serve YOLOv5 - serve model as Rest API service. You can run pretrained model, use custom model weights trained in Supervisely as well as weights trained outside (just upload weights file to Team Files). Thus other apps from Ecosystem can get predictions from the deployed model. Also developers can send inference requiests in a few lines of python code. Apply NN to images project - app allows to play with different inference options and visualize predictions in real time. Once you choose inference settings you can apply model to all images in your project to visually analise predictions and perform automatic data pre-labeling. NN Image Labeling - integrate any deployd NN to Supervisely Image Labeling UI. Configure inference settings and model output classes. Press Apply button (or use hotkey) and detections with their confidences will immediately appear on the image. Convert Supervisely to YOLO v5 format - export labeled images project in yolov5 compatible format. Convert YOLO v5 to Supervisely format - import images and yolov5 annotatons to Supervisely.","title":"\ud83c\udf89 YOLO v5 Apps Collection"},{"location":"tutorials/supervisely-ecosystem/#for-developers","text":"you can use sources of Serve YOLOv5 app as example of how to prepare weights, initialize model and apply it to a folder with images (or to images URLs) This apps collection is based on the original YOLOv5 release v5.0 . Once a next official release is available, all apps will be synchronized with it and also released with the new versions. Before running any app you can choose what version to use. Also Supervisely Team will pull updates from original master branch from time to time.","title":"For Developers"},{"location":"tutorials/supervisely-ecosystem/#contact-questions-suggestions","text":"for technical support please leave issues, questions or suggestions to original YOLOv5 repo with the prefix [Supervisely] . Our team will try to help. also we can chat in slack channel","title":"Contact &amp; Questions &amp; Suggestions"},{"location":"tutorials/test-time-augmentation/","text":"\ud83d\udcda This guide explains how to use Test Time Augmentation (TTA) during testing and inference for improved mAP and Recall with YOLOv5 \ud83d\ude80. Before You Start Clone this repo and install requirements.txt dependencies, including Python>=3.8 and PyTorch>=1.7 . git clone https://github.com/ultralytics/yolov5 # clone repo cd yolov5 pip install -r requirements.txt # install requirements.txt Test Normally Before trying TTA we want to establish the baseline performance by testing under default settings. This command tests YOLOv5x on COCO val2017 at image size 640 pixels. yolov5x.pt is the largest and most accurate model available. Other options are yolov5s.pt , yolov5m.pt and yolov5l.pt , or you own checkpoint from training a custom dataset ./weights/best.pt . For details on all available models please see our README table . $ python test.py --weights yolov5x.pt --data coco.yaml --img 640 Output: Namespace(augment=False, batch_size=32, conf_thres=0.001, data='./data/coco.yaml', device='', img_size=640, iou_thres=0.65, save_json=True, save_txt=False, single_cls=False, task='val', verbose=False, weights=['yolov5x.pt']) Using CUDA device0 _CudaDeviceProperties(name='Tesla P100-PCIE-16GB', total_memory=16280MB) Fusing layers... Model Summary: 284 layers, 8.89222e+07 parameters, 0 gradients Scanning labels ../coco/labels/val2017.cache (4952 found, 0 missing, 48 empty, 0 duplicate, for 5000 images): 5000it [00:00, 17761.74it/s] Class Images Targets P R mAP@.5 mAP@.5:.95: 100% 157/157 [02:34<00:00, 1.02it/s] all 5e+03 3.63e+04 0.409 0.754 0.669 0.476 Speed: 23.6/1.6/25.2 ms inference/NMS/total per 640x640 image at batch-size 32 < ---------- baseline speed COCO mAP with pycocotools... saving detections_val2017__results.json... Average Precision (AP) @[ IoU=0.50:0.95 | area= all | maxDets=100 ] = 0.492 < ---------- baseline mAP Average Precision (AP) @[ IoU=0.50 | area= all | maxDets=100 ] = 0.676 Average Precision (AP) @[ IoU=0.75 | area= all | maxDets=100 ] = 0.534 Average Precision (AP) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.318 Average Precision (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.541 Average Precision (AP) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.633 Average Recall (AR) @[ IoU=0.50:0.95 | area= all | maxDets= 1 ] = 0.376 Average Recall (AR) @[ IoU=0.50:0.95 | area= all | maxDets= 10 ] = 0.616 Average Recall (AR) @[ IoU=0.50:0.95 | area= all | maxDets=100 ] = 0.670 < ---------- baseline mAR Average Recall (AR) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.493 Average Recall (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.723 Average Recall (AR) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.812 Test with TTA Append --augment to any existing test.py command to enable TTA, and increase the image size by about 30% for improved results. Note that inference with TTA enabled will typically take about 2-3X the time of normal inference as the images are being left-right flipped and processed at 3 different resolutions, with the outputs merged before NMS. Part of the speed decrease is simply due to larger image sizes (832 vs 640), while part is due to the actual TTA operations. $ python test.py --weights yolov5x.pt --data coco.yaml --img 832 --augment Output: Namespace(augment=True, batch_size=32, conf_thres=0.001, data='./data/coco.yaml', device='', img_size=832, iou_thres=0.65, save_json=True, save_txt=False, single_cls=False, task='val', verbose=False, weights=['yolov5x.pt']) Using CUDA device0 _CudaDeviceProperties(name='Tesla P100-PCIE-16GB', total_memory=16280MB) Fusing layers... Model Summary: 284 layers, 8.89222e+07 parameters, 0 gradients Scanning labels ../coco/labels/val2017.cache (4952 found, 0 missing, 48 empty, 0 duplicate, for 5000 images): 5000it [00:00, 17064.47it/s] Class Images Targets P R mAP@.5 mAP@.5:.95: 100% 157/157 [07:54<00:00, 3.02s/it] all 5e+03 3.63e+04 0.309 0.807 0.682 0.492 Speed: 85.1/3.6/88.7 ms inference/NMS/total per 832x832 image at batch-size 32 < ---------- reduced speed COCO mAP with pycocotools... saving detections_val2017__results.json... ... Average Precision (AP) @[ IoU=0.50:0.95 | area= all | maxDets=100 ] = 0.508 < ---------- improved mAP Average Precision (AP) @[ IoU=0.50 | area= all | maxDets=100 ] = 0.689 Average Precision (AP) @[ IoU=0.75 | area= all | maxDets=100 ] = 0.556 Average Precision (AP) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.346 Average Precision (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.557 Average Precision (AP) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.648 Average Recall (AR) @[ IoU=0.50:0.95 | area= all | maxDets= 1 ] = 0.385 Average Recall (AR) @[ IoU=0.50:0.95 | area= all | maxDets= 10 ] = 0.635 Average Recall (AR) @[ IoU=0.50:0.95 | area= all | maxDets=100 ] = 0.696 < ---------- improved mAR Average Recall (AR) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.536 Average Recall (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.740 Average Recall (AR) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.826 Inference with TTA detect.py TTA inference operates identically to test.py TTA: simply append --augment to any existing detect.py command: $ python detect.py --weights yolov5s.pt --img 832 --source ./inference/images/ --augment Output: Namespace(agnostic_nms=False, augment=True, classes=None, conf_thres=0.25, device='', img_size=832, iou_thres=0.45, output='inference/output', save_txt=False, source='./inference/images/', update=False, view_img=False, weights=['yolov5s.pt']) Using CUDA device0 _CudaDeviceProperties(name='Tesla P100-PCIE-16GB', total_memory=16280MB) Fusing layers... Model Summary: 140 layers, 7.45958e+06 parameters, 0 gradients image 1/2 /yolov5/inference/images/bus.jpg: 832x640 4 persons, 1 buss, Done. (0.037s) image 2/2 /yolov5/images/zidane.jpg: 512x832 2 persons, 3 ties, Done. (0.036s) Results saved to inference/output Done. (0.186s) PyTorch Hub TTA TTA is automatically integrated into all YOLOv5 PyTorch Hub models, and can be accessed by passing augment=True at inference time. import torch # Model model = torch.hub.load('ultralytics/yolov5', 'yolov5s') # Images dir = 'https://github.com/ultralytics/yolov5/raw/master/data/images/' imgs = [dir + f for f in ('zidane.jpg', 'bus.jpg')] # batch of images # Inference results = model(imgs, augment=True) # TTA inference Environments YOLOv5 may be run in any of the following up-to-date verified environments (with all dependencies including CUDA / CUDNN , Python and PyTorch preinstalled): Google Colab and Kaggle notebooks with free GPU: Google Cloud Deep Learning VM. See GCP Quickstart Guide Amazon Deep Learning AMI. See AWS Quickstart Guide Docker Image . See Docker Quickstart Guide Status If this badge is green, all YOLOv5 GitHub Actions Continuous Integration (CI) tests are currently passing. CI tests verify correct operation of YOLOv5 training ( train.py ), testing ( test.py ), inference ( detect.py ) and export ( export.py ) on MacOS, Windows, and Ubuntu every 24 hours and on every commit.","title":"Test-Time Augmentation (TTA)"},{"location":"tutorials/test-time-augmentation/#before-you-start","text":"Clone this repo and install requirements.txt dependencies, including Python>=3.8 and PyTorch>=1.7 . git clone https://github.com/ultralytics/yolov5 # clone repo cd yolov5 pip install -r requirements.txt # install requirements.txt","title":"Before You Start"},{"location":"tutorials/test-time-augmentation/#test-normally","text":"Before trying TTA we want to establish the baseline performance by testing under default settings. This command tests YOLOv5x on COCO val2017 at image size 640 pixels. yolov5x.pt is the largest and most accurate model available. Other options are yolov5s.pt , yolov5m.pt and yolov5l.pt , or you own checkpoint from training a custom dataset ./weights/best.pt . For details on all available models please see our README table . $ python test.py --weights yolov5x.pt --data coco.yaml --img 640 Output: Namespace(augment=False, batch_size=32, conf_thres=0.001, data='./data/coco.yaml', device='', img_size=640, iou_thres=0.65, save_json=True, save_txt=False, single_cls=False, task='val', verbose=False, weights=['yolov5x.pt']) Using CUDA device0 _CudaDeviceProperties(name='Tesla P100-PCIE-16GB', total_memory=16280MB) Fusing layers... Model Summary: 284 layers, 8.89222e+07 parameters, 0 gradients Scanning labels ../coco/labels/val2017.cache (4952 found, 0 missing, 48 empty, 0 duplicate, for 5000 images): 5000it [00:00, 17761.74it/s] Class Images Targets P R mAP@.5 mAP@.5:.95: 100% 157/157 [02:34<00:00, 1.02it/s] all 5e+03 3.63e+04 0.409 0.754 0.669 0.476 Speed: 23.6/1.6/25.2 ms inference/NMS/total per 640x640 image at batch-size 32 < ---------- baseline speed COCO mAP with pycocotools... saving detections_val2017__results.json... Average Precision (AP) @[ IoU=0.50:0.95 | area= all | maxDets=100 ] = 0.492 < ---------- baseline mAP Average Precision (AP) @[ IoU=0.50 | area= all | maxDets=100 ] = 0.676 Average Precision (AP) @[ IoU=0.75 | area= all | maxDets=100 ] = 0.534 Average Precision (AP) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.318 Average Precision (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.541 Average Precision (AP) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.633 Average Recall (AR) @[ IoU=0.50:0.95 | area= all | maxDets= 1 ] = 0.376 Average Recall (AR) @[ IoU=0.50:0.95 | area= all | maxDets= 10 ] = 0.616 Average Recall (AR) @[ IoU=0.50:0.95 | area= all | maxDets=100 ] = 0.670 < ---------- baseline mAR Average Recall (AR) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.493 Average Recall (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.723 Average Recall (AR) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.812","title":"Test Normally"},{"location":"tutorials/test-time-augmentation/#test-with-tta","text":"Append --augment to any existing test.py command to enable TTA, and increase the image size by about 30% for improved results. Note that inference with TTA enabled will typically take about 2-3X the time of normal inference as the images are being left-right flipped and processed at 3 different resolutions, with the outputs merged before NMS. Part of the speed decrease is simply due to larger image sizes (832 vs 640), while part is due to the actual TTA operations. $ python test.py --weights yolov5x.pt --data coco.yaml --img 832 --augment Output: Namespace(augment=True, batch_size=32, conf_thres=0.001, data='./data/coco.yaml', device='', img_size=832, iou_thres=0.65, save_json=True, save_txt=False, single_cls=False, task='val', verbose=False, weights=['yolov5x.pt']) Using CUDA device0 _CudaDeviceProperties(name='Tesla P100-PCIE-16GB', total_memory=16280MB) Fusing layers... Model Summary: 284 layers, 8.89222e+07 parameters, 0 gradients Scanning labels ../coco/labels/val2017.cache (4952 found, 0 missing, 48 empty, 0 duplicate, for 5000 images): 5000it [00:00, 17064.47it/s] Class Images Targets P R mAP@.5 mAP@.5:.95: 100% 157/157 [07:54<00:00, 3.02s/it] all 5e+03 3.63e+04 0.309 0.807 0.682 0.492 Speed: 85.1/3.6/88.7 ms inference/NMS/total per 832x832 image at batch-size 32 < ---------- reduced speed COCO mAP with pycocotools... saving detections_val2017__results.json... ... Average Precision (AP) @[ IoU=0.50:0.95 | area= all | maxDets=100 ] = 0.508 < ---------- improved mAP Average Precision (AP) @[ IoU=0.50 | area= all | maxDets=100 ] = 0.689 Average Precision (AP) @[ IoU=0.75 | area= all | maxDets=100 ] = 0.556 Average Precision (AP) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.346 Average Precision (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.557 Average Precision (AP) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.648 Average Recall (AR) @[ IoU=0.50:0.95 | area= all | maxDets= 1 ] = 0.385 Average Recall (AR) @[ IoU=0.50:0.95 | area= all | maxDets= 10 ] = 0.635 Average Recall (AR) @[ IoU=0.50:0.95 | area= all | maxDets=100 ] = 0.696 < ---------- improved mAR Average Recall (AR) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.536 Average Recall (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.740 Average Recall (AR) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.826","title":"Test with TTA"},{"location":"tutorials/test-time-augmentation/#inference-with-tta","text":"detect.py TTA inference operates identically to test.py TTA: simply append --augment to any existing detect.py command: $ python detect.py --weights yolov5s.pt --img 832 --source ./inference/images/ --augment Output: Namespace(agnostic_nms=False, augment=True, classes=None, conf_thres=0.25, device='', img_size=832, iou_thres=0.45, output='inference/output', save_txt=False, source='./inference/images/', update=False, view_img=False, weights=['yolov5s.pt']) Using CUDA device0 _CudaDeviceProperties(name='Tesla P100-PCIE-16GB', total_memory=16280MB) Fusing layers... Model Summary: 140 layers, 7.45958e+06 parameters, 0 gradients image 1/2 /yolov5/inference/images/bus.jpg: 832x640 4 persons, 1 buss, Done. (0.037s) image 2/2 /yolov5/images/zidane.jpg: 512x832 2 persons, 3 ties, Done. (0.036s) Results saved to inference/output Done. (0.186s)","title":"Inference with TTA"},{"location":"tutorials/test-time-augmentation/#pytorch-hub-tta","text":"TTA is automatically integrated into all YOLOv5 PyTorch Hub models, and can be accessed by passing augment=True at inference time. import torch # Model model = torch.hub.load('ultralytics/yolov5', 'yolov5s') # Images dir = 'https://github.com/ultralytics/yolov5/raw/master/data/images/' imgs = [dir + f for f in ('zidane.jpg', 'bus.jpg')] # batch of images # Inference results = model(imgs, augment=True) # TTA inference","title":"PyTorch Hub TTA"},{"location":"tutorials/test-time-augmentation/#environments","text":"YOLOv5 may be run in any of the following up-to-date verified environments (with all dependencies including CUDA / CUDNN , Python and PyTorch preinstalled): Google Colab and Kaggle notebooks with free GPU: Google Cloud Deep Learning VM. See GCP Quickstart Guide Amazon Deep Learning AMI. See AWS Quickstart Guide Docker Image . See Docker Quickstart Guide","title":"Environments"},{"location":"tutorials/test-time-augmentation/#status","text":"If this badge is green, all YOLOv5 GitHub Actions Continuous Integration (CI) tests are currently passing. CI tests verify correct operation of YOLOv5 training ( train.py ), testing ( test.py ), inference ( detect.py ) and export ( export.py ) on MacOS, Windows, and Ubuntu every 24 hours and on every commit.","title":"Status"},{"location":"tutorials/torchscript-onnx-coreml-export/","text":"\ud83d\udcda This guide explains how to export a trained YOLOv5 \ud83d\ude80 model from PyTorch to ONNX and TorchScript formats. Before You Start Clone this repo and install requirements.txt dependencies, including Python>=3.8 and PyTorch==1.7 . git clone https://github.com/ultralytics/yolov5 # clone repo cd yolov5 pip install -r requirements.txt # base requirements pip install coremltools>=4.1 onnx>=1.9.0 scikit-learn==0.19.2 # export requirements Export a Trained YOLOv5 Model This command exports a pretrained YOLOv5s model to ONNX, TorchScript and CoreML formats. yolov5s.pt is the lightest and fastest model available. Other options are yolov5m.pt , yolov5l.pt and yolov5x.pt , or you own checkpoint from training a custom dataset runs/exp0/weights/best.pt . For details on all available models please see our README table . python models/export.py --weights yolov5s.pt --img 640 --batch 1 # export at 640x640 with batch size 1 Output: Namespace(batch_size=1, device='cpu', dynamic=False, half=False, img_size=[640, 640], include=['torchscript', 'onnx', 'coreml'], inplace=False, optimize=False, simplify=False, train=True, weights='./yolov5s.pt') YOLOv5 \ud83d\ude80 v5.0-87-gf12cef8 torch 1.8.1+cu101 CPU Fusing layers... Model Summary: 224 layers, 7266973 parameters, 0 gradients PyTorch: starting from ./yolov5s.pt (14.8 MB) TorchScript: starting export with torch 1.8.1+cu101... TorchScript: export success, saved as ./yolov5s.torchscript.pt (29.4 MB) ONNX: starting export with onnx 1.9.0... ONNX: export success, saved as ./yolov5s.onnx (29.1 MB) CoreML: starting export with coremltools 4.1... CoreML: export success, saved as ./yolov5s.mlmodel (29.1 MB) Export complete (10.40s). Visualize with https://github.com/lutzroeder/netron. The 3 exported models will be saved alongside the original PyTorch model: Netron Viewer is recommended for visualizing exported models: TensorRT Deployment For deployment of YOLOv5 from PyTorch *.pt weights to NVIDIA TensorRT see https://github.com/wang-xinyu/tensorrtx. Environments YOLOv5 may be run in any of the following up-to-date verified environments (with all dependencies including CUDA / CUDNN , Python and PyTorch preinstalled): Google Colab and Kaggle notebooks with free GPU: Google Cloud Deep Learning VM. See GCP Quickstart Guide Amazon Deep Learning AMI. See AWS Quickstart Guide Docker Image . See Docker Quickstart Guide Status If this badge is green, all YOLOv5 GitHub Actions Continuous Integration (CI) tests are currently passing. CI tests verify correct operation of YOLOv5 training ( train.py ), testing ( test.py ), inference ( detect.py ) and export ( export.py ) on MacOS, Windows, and Ubuntu every 24 hours and on every commit.","title":"TorchScript, ONNX, CoreML Export"},{"location":"tutorials/torchscript-onnx-coreml-export/#before-you-start","text":"Clone this repo and install requirements.txt dependencies, including Python>=3.8 and PyTorch==1.7 . git clone https://github.com/ultralytics/yolov5 # clone repo cd yolov5 pip install -r requirements.txt # base requirements pip install coremltools>=4.1 onnx>=1.9.0 scikit-learn==0.19.2 # export requirements","title":"Before You Start"},{"location":"tutorials/torchscript-onnx-coreml-export/#export-a-trained-yolov5-model","text":"This command exports a pretrained YOLOv5s model to ONNX, TorchScript and CoreML formats. yolov5s.pt is the lightest and fastest model available. Other options are yolov5m.pt , yolov5l.pt and yolov5x.pt , or you own checkpoint from training a custom dataset runs/exp0/weights/best.pt . For details on all available models please see our README table . python models/export.py --weights yolov5s.pt --img 640 --batch 1 # export at 640x640 with batch size 1 Output: Namespace(batch_size=1, device='cpu', dynamic=False, half=False, img_size=[640, 640], include=['torchscript', 'onnx', 'coreml'], inplace=False, optimize=False, simplify=False, train=True, weights='./yolov5s.pt') YOLOv5 \ud83d\ude80 v5.0-87-gf12cef8 torch 1.8.1+cu101 CPU Fusing layers... Model Summary: 224 layers, 7266973 parameters, 0 gradients PyTorch: starting from ./yolov5s.pt (14.8 MB) TorchScript: starting export with torch 1.8.1+cu101... TorchScript: export success, saved as ./yolov5s.torchscript.pt (29.4 MB) ONNX: starting export with onnx 1.9.0... ONNX: export success, saved as ./yolov5s.onnx (29.1 MB) CoreML: starting export with coremltools 4.1... CoreML: export success, saved as ./yolov5s.mlmodel (29.1 MB) Export complete (10.40s). Visualize with https://github.com/lutzroeder/netron. The 3 exported models will be saved alongside the original PyTorch model: Netron Viewer is recommended for visualizing exported models:","title":"Export a Trained YOLOv5 Model"},{"location":"tutorials/torchscript-onnx-coreml-export/#tensorrt-deployment","text":"For deployment of YOLOv5 from PyTorch *.pt weights to NVIDIA TensorRT see https://github.com/wang-xinyu/tensorrtx.","title":"TensorRT Deployment"},{"location":"tutorials/torchscript-onnx-coreml-export/#environments","text":"YOLOv5 may be run in any of the following up-to-date verified environments (with all dependencies including CUDA / CUDNN , Python and PyTorch preinstalled): Google Colab and Kaggle notebooks with free GPU: Google Cloud Deep Learning VM. See GCP Quickstart Guide Amazon Deep Learning AMI. See AWS Quickstart Guide Docker Image . See Docker Quickstart Guide","title":"Environments"},{"location":"tutorials/torchscript-onnx-coreml-export/#status","text":"If this badge is green, all YOLOv5 GitHub Actions Continuous Integration (CI) tests are currently passing. CI tests verify correct operation of YOLOv5 training ( train.py ), testing ( test.py ), inference ( detect.py ) and export ( export.py ) on MacOS, Windows, and Ubuntu every 24 hours and on every commit.","title":"Status"},{"location":"tutorials/train-custom-datasets/","text":"\ud83d\udcda This guide explains how to train your own custom dataset with YOLOv5 \ud83d\ude80. Before You Start Clone this repo, download tutorial dataset, and install requirements.txt dependencies, including Python>=3.8 and PyTorch>=1.7 . $ git clone https://github.com/ultralytics/yolov5 # clone repo $ cd yolov5 $ pip install -r requirements.txt # install Train On Custom Data 1. Create dataset.yaml COCO128 is a small tutorial dataset composed of the first 128 images in COCO train2017. These same 128 images are used for both training and validation to verify our training pipeline is capable of overfitting. data/coco128.yaml , shown below, is the dataset configuration file that defines 1) an optional download command/URL for auto-downloading, 2) a path to a directory of training images (or path to a *.txt file with a list of training images), 3) the same for our validation images, 4) the number of classes, 5) a list of class names: # train and val data as 1) directory: path/images/, 2) file: path/images.txt, or 3) list: [path1/images/, path2/images/] train: ../coco128/images/train2017/ val: ../coco128/images/train2017/ # number of classes nc: 80 # class names names: ['person', 'bicycle', 'car', 'motorcycle', 'airplane', 'bus', 'train', 'truck', 'boat', 'traffic light', 'fire hydrant', 'stop sign', 'parking meter', 'bench', 'bird', 'cat', 'dog', 'horse', 'sheep', 'cow', 'elephant', 'bear', 'zebra', 'giraffe', 'backpack', 'umbrella', 'handbag', 'tie', 'suitcase', 'frisbee', 'skis', 'snowboard', 'sports ball', 'kite', 'baseball bat', 'baseball glove', 'skateboard', 'surfboard', 'tennis racket', 'bottle', 'wine glass', 'cup', 'fork', 'knife', 'spoon', 'bowl', 'banana', 'apple', 'sandwich', 'orange', 'broccoli', 'carrot', 'hot dog', 'pizza', 'donut', 'cake', 'chair', 'couch', 'potted plant', 'bed', 'dining table', 'toilet', 'tv', 'laptop', 'mouse', 'remote', 'keyboard', 'cell phone', 'microwave', 'oven', 'toaster', 'sink', 'refrigerator', 'book', 'clock', 'vase', 'scissors', 'teddy bear', 'hair drier', 'toothbrush'] 2. Create Labels After using a tool like CVAT , makesense.ai or Labelbox to label your images, export your labels to YOLO format , with one *.txt file per image (if no objects in image, no *.txt file is required). The *.txt file specifications are: One row per object Each row is class x_center y_center width height format. Box coordinates must be in normalized xywh format (from 0 - 1). If your boxes are in pixels, divide x_center and width by image width, and y_center and height by image height. Class numbers are zero-indexed (start from 0). The label file corresponding to the above image contains 2 persons (class 0 ) and a tie (class 27 ): 3. Organize Directories Organize your train and val images and labels according to the example below. In this example we assume /coco128 is next to the /yolov5 directory. YOLOv5 locates labels automatically for each image by replacing the last instance of /images/ in each image path with /labels/ . For example: dataset/images/im0.jpg # image dataset/labels/im0.txt # label 4. Select a Model Select a pretrained model to start training from. Here we select YOLOv5s , the smallest and fastest model available. See our README table for a full comparison of all models. 5. Train Train a YOLOv5s model on COCO128 by specifying dataset, batch-size, image size and either pretrained --weights yolov5s.pt (recommended), or randomly initialized --weights '' --cfg yolov5s.yaml (not recommended). Pretrained weights are auto-downloaded from the latest YOLOv5 release . # Train YOLOv5s on COCO128 for 5 epochs $ python train.py --img 640 --batch 16 --epochs 5 --data coco128.yaml --weights yolov5s.pt All training results are saved to runs/train/ with incrementing run directories, i.e. runs/train/exp2 , runs/train/exp3 etc. For more details see the Training section of our Google Colab Notebook. Visualize Weights & Biases Logging (\ud83d\ude80 NEW) Weights & Biases (W&B) is now integrated with YOLOv5 for real-time visualization and cloud logging of training runs. This allows for better run comparison and introspection, as well improved visibility and collaboration among team members. To enable W&B logging install wandb , and then train normally (you will be guided setup on first use). $ pip install wandb During training you will see live updates at https://wandb.ai , and you can create Detailed Reports of your results using the W&B Reports tool. Local Logging All results are logged by default to runs/train , with a new experiment directory created for each new training as runs/train/exp2 , runs/train/exp3 , etc. View train and test jpgs to see mosaics, labels, predictions and augmentation effects. Note a Mosaic Dataloader is used for training (shown below), a new concept developed by Ultralytics and first featured in YOLOv4 . train_batch0.jpg shows train batch 0 mosaics and labels: test_batch0_labels.jpg shows test batch 0 labels: test_batch0_pred.jpg shows test batch 0 predictions : Training losses and performance metrics are also logged to Tensorboard and a custom results.txt logfile which is plotted as results.png (below) after training completes. Here we show YOLOv5s trained on COCO128 to 300 epochs, starting from scratch (blue), and from pretrained --weights yolov5s.pt (orange). from utils.plots import plot_results plot_results(save_dir='runs/train/exp') # plot results.txt as results.png Environments YOLOv5 may be run in any of the following up-to-date verified environments (with all dependencies including CUDA / CUDNN , Python and PyTorch preinstalled): Google Colab and Kaggle notebooks with free GPU: Google Cloud Deep Learning VM. See GCP Quickstart Guide Amazon Deep Learning AMI. See AWS Quickstart Guide Docker Image . See Docker Quickstart Guide Status If this badge is green, all YOLOv5 GitHub Actions Continuous Integration (CI) tests are currently passing. CI tests verify correct operation of YOLOv5 training ( train.py ), testing ( test.py ), inference ( detect.py ) and export ( export.py ) on MacOS, Windows, and Ubuntu every 24 hours and on every commit.","title":"Train Custom Data \ud83d\udccc"},{"location":"tutorials/train-custom-datasets/#before-you-start","text":"Clone this repo, download tutorial dataset, and install requirements.txt dependencies, including Python>=3.8 and PyTorch>=1.7 . $ git clone https://github.com/ultralytics/yolov5 # clone repo $ cd yolov5 $ pip install -r requirements.txt # install","title":"Before You Start"},{"location":"tutorials/train-custom-datasets/#train-on-custom-data","text":"","title":"Train On Custom Data"},{"location":"tutorials/train-custom-datasets/#1-create-datasetyaml","text":"COCO128 is a small tutorial dataset composed of the first 128 images in COCO train2017. These same 128 images are used for both training and validation to verify our training pipeline is capable of overfitting. data/coco128.yaml , shown below, is the dataset configuration file that defines 1) an optional download command/URL for auto-downloading, 2) a path to a directory of training images (or path to a *.txt file with a list of training images), 3) the same for our validation images, 4) the number of classes, 5) a list of class names: # train and val data as 1) directory: path/images/, 2) file: path/images.txt, or 3) list: [path1/images/, path2/images/] train: ../coco128/images/train2017/ val: ../coco128/images/train2017/ # number of classes nc: 80 # class names names: ['person', 'bicycle', 'car', 'motorcycle', 'airplane', 'bus', 'train', 'truck', 'boat', 'traffic light', 'fire hydrant', 'stop sign', 'parking meter', 'bench', 'bird', 'cat', 'dog', 'horse', 'sheep', 'cow', 'elephant', 'bear', 'zebra', 'giraffe', 'backpack', 'umbrella', 'handbag', 'tie', 'suitcase', 'frisbee', 'skis', 'snowboard', 'sports ball', 'kite', 'baseball bat', 'baseball glove', 'skateboard', 'surfboard', 'tennis racket', 'bottle', 'wine glass', 'cup', 'fork', 'knife', 'spoon', 'bowl', 'banana', 'apple', 'sandwich', 'orange', 'broccoli', 'carrot', 'hot dog', 'pizza', 'donut', 'cake', 'chair', 'couch', 'potted plant', 'bed', 'dining table', 'toilet', 'tv', 'laptop', 'mouse', 'remote', 'keyboard', 'cell phone', 'microwave', 'oven', 'toaster', 'sink', 'refrigerator', 'book', 'clock', 'vase', 'scissors', 'teddy bear', 'hair drier', 'toothbrush']","title":"1. Create dataset.yaml"},{"location":"tutorials/train-custom-datasets/#2-create-labels","text":"After using a tool like CVAT , makesense.ai or Labelbox to label your images, export your labels to YOLO format , with one *.txt file per image (if no objects in image, no *.txt file is required). The *.txt file specifications are: One row per object Each row is class x_center y_center width height format. Box coordinates must be in normalized xywh format (from 0 - 1). If your boxes are in pixels, divide x_center and width by image width, and y_center and height by image height. Class numbers are zero-indexed (start from 0). The label file corresponding to the above image contains 2 persons (class 0 ) and a tie (class 27 ):","title":"2. Create Labels"},{"location":"tutorials/train-custom-datasets/#3-organize-directories","text":"Organize your train and val images and labels according to the example below. In this example we assume /coco128 is next to the /yolov5 directory. YOLOv5 locates labels automatically for each image by replacing the last instance of /images/ in each image path with /labels/ . For example: dataset/images/im0.jpg # image dataset/labels/im0.txt # label","title":"3. Organize Directories"},{"location":"tutorials/train-custom-datasets/#4-select-a-model","text":"Select a pretrained model to start training from. Here we select YOLOv5s , the smallest and fastest model available. See our README table for a full comparison of all models.","title":"4. Select a Model"},{"location":"tutorials/train-custom-datasets/#5-train","text":"Train a YOLOv5s model on COCO128 by specifying dataset, batch-size, image size and either pretrained --weights yolov5s.pt (recommended), or randomly initialized --weights '' --cfg yolov5s.yaml (not recommended). Pretrained weights are auto-downloaded from the latest YOLOv5 release . # Train YOLOv5s on COCO128 for 5 epochs $ python train.py --img 640 --batch 16 --epochs 5 --data coco128.yaml --weights yolov5s.pt All training results are saved to runs/train/ with incrementing run directories, i.e. runs/train/exp2 , runs/train/exp3 etc. For more details see the Training section of our Google Colab Notebook.","title":"5. Train"},{"location":"tutorials/train-custom-datasets/#visualize","text":"","title":"Visualize"},{"location":"tutorials/train-custom-datasets/#weights-biases-logging-new","text":"Weights & Biases (W&B) is now integrated with YOLOv5 for real-time visualization and cloud logging of training runs. This allows for better run comparison and introspection, as well improved visibility and collaboration among team members. To enable W&B logging install wandb , and then train normally (you will be guided setup on first use). $ pip install wandb During training you will see live updates at https://wandb.ai , and you can create Detailed Reports of your results using the W&B Reports tool.","title":"Weights &amp; Biases Logging (\ud83d\ude80 NEW)"},{"location":"tutorials/train-custom-datasets/#local-logging","text":"All results are logged by default to runs/train , with a new experiment directory created for each new training as runs/train/exp2 , runs/train/exp3 , etc. View train and test jpgs to see mosaics, labels, predictions and augmentation effects. Note a Mosaic Dataloader is used for training (shown below), a new concept developed by Ultralytics and first featured in YOLOv4 . train_batch0.jpg shows train batch 0 mosaics and labels: test_batch0_labels.jpg shows test batch 0 labels: test_batch0_pred.jpg shows test batch 0 predictions : Training losses and performance metrics are also logged to Tensorboard and a custom results.txt logfile which is plotted as results.png (below) after training completes. Here we show YOLOv5s trained on COCO128 to 300 epochs, starting from scratch (blue), and from pretrained --weights yolov5s.pt (orange). from utils.plots import plot_results plot_results(save_dir='runs/train/exp') # plot results.txt as results.png","title":"Local Logging"},{"location":"tutorials/train-custom-datasets/#environments","text":"YOLOv5 may be run in any of the following up-to-date verified environments (with all dependencies including CUDA / CUDNN , Python and PyTorch preinstalled): Google Colab and Kaggle notebooks with free GPU: Google Cloud Deep Learning VM. See GCP Quickstart Guide Amazon Deep Learning AMI. See AWS Quickstart Guide Docker Image . See Docker Quickstart Guide","title":"Environments"},{"location":"tutorials/train-custom-datasets/#status","text":"If this badge is green, all YOLOv5 GitHub Actions Continuous Integration (CI) tests are currently passing. CI tests verify correct operation of YOLOv5 training ( train.py ), testing ( test.py ), inference ( detect.py ) and export ( export.py ) on MacOS, Windows, and Ubuntu every 24 hours and on every commit.","title":"Status"},{"location":"tutorials/training-tips-best-results/","text":"\ud83d\udc4b Hello! \ud83d\udcda This guide explains how to produce the best mAP and training results with YOLOv5 \ud83d\ude80. Most of the time good results can be obtained with no changes to the models or training settings, provided your dataset is sufficiently large and well labelled . If at first you don't get good results, there are steps you might be able to take to improve, but we always recommend users first train with all default settings before considering any changes. This helps establish a performance baseline and spot areas for improvement. If you have questions about your training results we recommend you provide the maximum amount of information possible if you expect a helpful response, including results plots (train losses, val losses, P, R, mAP), PR curve, confusion matrix, training mosaics, test results and dataset statistics images such as labels.png. All of these are located in your project/name directory, typically yolov5/runs/train/exp . We've put together a full guide for users looking to get the best results on their YOLOv5 trainings below. Dataset Images per class. \u22651.5k images per class Instances per class. \u226510k instances (labeled objects) per class total Image variety. Must be representative of deployed environment. For real-world use cases we recommend images from different times of day, different seasons, different weather, different lighting, different angles, different sources (scraped online, collected locally, different cameras) etc. Label consistency. All instances of all classes in all images must be labelled. Partial labelling will not work. Label accuracy. Labels must closely enclose each object. No space should exist between an object and it's bounding box. No objects should be missing a label. Background images. Background images are images with no objects that are added to a dataset to reduce False Positives (FP). We recommend about 0-10% background images to help reduce FPs (COCO has 1000 background images for reference, 1% of the total). Model Selection Larger models like YOLOv5x and YOLOv5x6 will produce better results in nearly all cases, but have more parameters, require more CUDA memory to train, and are slower to run. For mobile deployments we recommend YOLOv5s/m, for cloud deployments we recommend YOLOv5l/x. See our README table for a full comparison of all models. Start from Pretrained weights. Recommended for small to medium sized datasets (i.e. VOC, VisDrone, GlobalWheat). Pass the name of the model to the --weights argument. Models download automatically from the latest YOLOv5 release . python train.py --data custom.yaml --weights yolov5s.pt yolov5m.pt yolov5l.pt yolov5x.pt custom_pretrained.pt Start from Scratch. Recommended for large datasets (i.e. COCO, Objects365, OIv6). Pass the model architecture yaml you are interested in, along with an empty --weights '' argument: python train.py --data custom.yaml --weights '' --cfg yolov5s.yaml yolov5m.yaml yolov5l.yaml yolov5x.yaml Training Settings Before modifying anything, first train with default settings to establish a performance baseline . A full list of train.py settings can be found in the train.py argparser. Epochs. Start with 300 epochs. If this overfits early then you can reduce epochs. If overfitting does not occur after 300 epochs, train longer, i.e. 600, 1200 etc epochs. Image size. COCO trains at native resolution of --img 640 , though due to the high amount of small objects in the dataset it can benefit from training at higher resolutions such as --img 1280 . If there are many small objects then custom datasets will benefit from training at native or higher resolution. Best inference results are obtained at the same --img as the training was run at, i.e. if you train at --img 1280 you should also test and detect at --img 1280 . Batch size. Use the largest --batch-size that your hardware allows for. Small batch sizes produce poor batchnorm statistics and should be avoided. Hyperparameters. Default hyperparameters are in hyp.scratch.yaml . We recommend you train with default hyperparameters first before thinking of modifying any. In general, increasing augmentation hyperparameters will reduce and delay overfitting, allowing for longer trainings and higher final mAP. Reduction in loss component gain hyperparameters like hyp['obj'] will help reduce overfitting in those specific loss components. For an automated method of optimizing these hyperparameters, see our Hyperparameter Evolution Tutorial . Further Reading If you'd like to know more a good place to start is Karpathy's 'Recipe for Training Neural Networks', which has great ideas for training that apply broadly across all ML domains: http://karpathy.github.io/2019/04/25/recipe/","title":"Tips for Best Training Results \ud83d\udccc"},{"location":"tutorials/training-tips-best-results/#dataset","text":"Images per class. \u22651.5k images per class Instances per class. \u226510k instances (labeled objects) per class total Image variety. Must be representative of deployed environment. For real-world use cases we recommend images from different times of day, different seasons, different weather, different lighting, different angles, different sources (scraped online, collected locally, different cameras) etc. Label consistency. All instances of all classes in all images must be labelled. Partial labelling will not work. Label accuracy. Labels must closely enclose each object. No space should exist between an object and it's bounding box. No objects should be missing a label. Background images. Background images are images with no objects that are added to a dataset to reduce False Positives (FP). We recommend about 0-10% background images to help reduce FPs (COCO has 1000 background images for reference, 1% of the total).","title":"Dataset"},{"location":"tutorials/training-tips-best-results/#model-selection","text":"Larger models like YOLOv5x and YOLOv5x6 will produce better results in nearly all cases, but have more parameters, require more CUDA memory to train, and are slower to run. For mobile deployments we recommend YOLOv5s/m, for cloud deployments we recommend YOLOv5l/x. See our README table for a full comparison of all models. Start from Pretrained weights. Recommended for small to medium sized datasets (i.e. VOC, VisDrone, GlobalWheat). Pass the name of the model to the --weights argument. Models download automatically from the latest YOLOv5 release . python train.py --data custom.yaml --weights yolov5s.pt yolov5m.pt yolov5l.pt yolov5x.pt custom_pretrained.pt Start from Scratch. Recommended for large datasets (i.e. COCO, Objects365, OIv6). Pass the model architecture yaml you are interested in, along with an empty --weights '' argument: python train.py --data custom.yaml --weights '' --cfg yolov5s.yaml yolov5m.yaml yolov5l.yaml yolov5x.yaml","title":"Model Selection"},{"location":"tutorials/training-tips-best-results/#training-settings","text":"Before modifying anything, first train with default settings to establish a performance baseline . A full list of train.py settings can be found in the train.py argparser. Epochs. Start with 300 epochs. If this overfits early then you can reduce epochs. If overfitting does not occur after 300 epochs, train longer, i.e. 600, 1200 etc epochs. Image size. COCO trains at native resolution of --img 640 , though due to the high amount of small objects in the dataset it can benefit from training at higher resolutions such as --img 1280 . If there are many small objects then custom datasets will benefit from training at native or higher resolution. Best inference results are obtained at the same --img as the training was run at, i.e. if you train at --img 1280 you should also test and detect at --img 1280 . Batch size. Use the largest --batch-size that your hardware allows for. Small batch sizes produce poor batchnorm statistics and should be avoided. Hyperparameters. Default hyperparameters are in hyp.scratch.yaml . We recommend you train with default hyperparameters first before thinking of modifying any. In general, increasing augmentation hyperparameters will reduce and delay overfitting, allowing for longer trainings and higher final mAP. Reduction in loss component gain hyperparameters like hyp['obj'] will help reduce overfitting in those specific loss components. For an automated method of optimizing these hyperparameters, see our Hyperparameter Evolution Tutorial .","title":"Training Settings"},{"location":"tutorials/training-tips-best-results/#further-reading","text":"If you'd like to know more a good place to start is Karpathy's 'Recipe for Training Neural Networks', which has great ideas for training that apply broadly across all ML domains: http://karpathy.github.io/2019/04/25/recipe/","title":"Further Reading"},{"location":"tutorials/transfer-learning-froze-layers/","text":"\ud83d\udcda This guide explains how to freeze YOLOv5 \ud83d\ude80 layers when transfer learning . Transfer learning is a useful way to quickly retrain a model on new data without having to retrain the entire network. Instead, part of the initial weights are frozen in place, and the rest of the weights are used to compute loss and are updated by the optimizer. This requires less resources than normal training and allows for faster training times, though it may also results in reductions to final trained accuracy. Before You Start Clone this repo and install requirements.txt dependencies, including Python>=3.8 and PyTorch>=1.7 . $ git clone https://github.com/ultralytics/yolov5 # clone repo $ cd yolov5 $ pip install wandb -qr requirements.txt # install requirements.txt Freeze Backbone All layers that match the freeze list in train.py will be frozen by setting their gradients to zero before training starts. https://github.com/ultralytics/yolov5/blob/58f8ba771e3712b525ca93a1ee66bc2b2df2092f/train.py#L83-L90 To see a list of module names: for k, v in model.named_parameters(): print(k) # Output model.0.conv.conv.weight model.0.conv.bn.weight model.0.conv.bn.bias model.1.conv.weight model.1.bn.weight model.1.bn.bias model.2.cv1.conv.weight model.2.cv1.bn.weight ... model.23.m.0.cv2.bn.weight model.23.m.0.cv2.bn.bias model.24.m.0.weight model.24.m.0.bias model.24.m.1.weight model.24.m.1.bias model.24.m.2.weight model.24.m.2.bias Looking at the model architecture we can see that the model backbone is layers 0-9: https://github.com/ultralytics/yolov5/blob/58f8ba771e3712b525ca93a1ee66bc2b2df2092f/models/yolov5s.yaml#L12-L48 so we define the freeze list to contain all modules with 'model.0.' - 'model.9.' in their names, and then we start training. freeze = ['model.%s.' % x for x in range(10)] # parameter names to freeze (full or partial) Freeze All Layers To freeze the full model except for the final output convolution layers in Detect(), we set freeze list to contain all modules with 'model.0.' - 'model.23.' in their names, and then we start training. freeze = ['model.%s.' % x for x in range(24)] # parameter names to freeze (full or partial) Results We trained YOLOv5m on VOC on both of the above scenarios, along with a default model (no freezing), starting from the official COCO pretrained --weights yolov5m.pt . The training command for all runs was: $ train.py --batch 48 --weights yolov5m.pt --data voc.yaml --epochs 50 --cache --img 512 --hyp hyp.finetune.yaml Accuracy Comparison The results show that freezing speeds up training, but reduces final accuracy slightly. A full W&B Report of the runs can be found at this link: https://wandb.ai/glenn-jocher/yolov5_tutorial_freeze/reports/Freezing-Layers-in-YOLOv5--VmlldzozMDk3NTg GPU Utilization Comparison Interestingly, the more modules are frozen the less GPU memory is required to train, and the lower GPU utilization. This indicates that larger models, or models trained at larger --image-size may benefit from freezing in order to train faster. Environments YOLOv5 may be run in any of the following up-to-date verified environments (with all dependencies including CUDA / CUDNN , Python and PyTorch preinstalled): Google Colab and Kaggle notebooks with free GPU: Google Cloud Deep Learning VM. See GCP Quickstart Guide Amazon Deep Learning AMI. See AWS Quickstart Guide Docker Image . See Docker Quickstart Guide Status If this badge is green, all YOLOv5 GitHub Actions Continuous Integration (CI) tests are currently passing. CI tests verify correct operation of YOLOv5 training ( train.py ), testing ( test.py ), inference ( detect.py ) and export ( export.py ) on MacOS, Windows, and Ubuntu every 24 hours and on every commit.","title":"Transfer Learning with Frozen Layers"},{"location":"tutorials/transfer-learning-froze-layers/#before-you-start","text":"Clone this repo and install requirements.txt dependencies, including Python>=3.8 and PyTorch>=1.7 . $ git clone https://github.com/ultralytics/yolov5 # clone repo $ cd yolov5 $ pip install wandb -qr requirements.txt # install requirements.txt","title":"Before You Start"},{"location":"tutorials/transfer-learning-froze-layers/#freeze-backbone","text":"All layers that match the freeze list in train.py will be frozen by setting their gradients to zero before training starts. https://github.com/ultralytics/yolov5/blob/58f8ba771e3712b525ca93a1ee66bc2b2df2092f/train.py#L83-L90 To see a list of module names: for k, v in model.named_parameters(): print(k) # Output model.0.conv.conv.weight model.0.conv.bn.weight model.0.conv.bn.bias model.1.conv.weight model.1.bn.weight model.1.bn.bias model.2.cv1.conv.weight model.2.cv1.bn.weight ... model.23.m.0.cv2.bn.weight model.23.m.0.cv2.bn.bias model.24.m.0.weight model.24.m.0.bias model.24.m.1.weight model.24.m.1.bias model.24.m.2.weight model.24.m.2.bias Looking at the model architecture we can see that the model backbone is layers 0-9: https://github.com/ultralytics/yolov5/blob/58f8ba771e3712b525ca93a1ee66bc2b2df2092f/models/yolov5s.yaml#L12-L48 so we define the freeze list to contain all modules with 'model.0.' - 'model.9.' in their names, and then we start training. freeze = ['model.%s.' % x for x in range(10)] # parameter names to freeze (full or partial)","title":"Freeze Backbone"},{"location":"tutorials/transfer-learning-froze-layers/#freeze-all-layers","text":"To freeze the full model except for the final output convolution layers in Detect(), we set freeze list to contain all modules with 'model.0.' - 'model.23.' in their names, and then we start training. freeze = ['model.%s.' % x for x in range(24)] # parameter names to freeze (full or partial)","title":"Freeze All Layers"},{"location":"tutorials/transfer-learning-froze-layers/#results","text":"We trained YOLOv5m on VOC on both of the above scenarios, along with a default model (no freezing), starting from the official COCO pretrained --weights yolov5m.pt . The training command for all runs was: $ train.py --batch 48 --weights yolov5m.pt --data voc.yaml --epochs 50 --cache --img 512 --hyp hyp.finetune.yaml","title":"Results"},{"location":"tutorials/transfer-learning-froze-layers/#accuracy-comparison","text":"The results show that freezing speeds up training, but reduces final accuracy slightly. A full W&B Report of the runs can be found at this link: https://wandb.ai/glenn-jocher/yolov5_tutorial_freeze/reports/Freezing-Layers-in-YOLOv5--VmlldzozMDk3NTg","title":"Accuracy Comparison"},{"location":"tutorials/transfer-learning-froze-layers/#gpu-utilization-comparison","text":"Interestingly, the more modules are frozen the less GPU memory is required to train, and the lower GPU utilization. This indicates that larger models, or models trained at larger --image-size may benefit from freezing in order to train faster.","title":"GPU Utilization Comparison"},{"location":"tutorials/transfer-learning-froze-layers/#environments","text":"YOLOv5 may be run in any of the following up-to-date verified environments (with all dependencies including CUDA / CUDNN , Python and PyTorch preinstalled): Google Colab and Kaggle notebooks with free GPU: Google Cloud Deep Learning VM. See GCP Quickstart Guide Amazon Deep Learning AMI. See AWS Quickstart Guide Docker Image . See Docker Quickstart Guide","title":"Environments"},{"location":"tutorials/transfer-learning-froze-layers/#status","text":"If this badge is green, all YOLOv5 GitHub Actions Continuous Integration (CI) tests are currently passing. CI tests verify correct operation of YOLOv5 training ( train.py ), testing ( test.py ), inference ( detect.py ) and export ( export.py ) on MacOS, Windows, and Ubuntu every 24 hours and on every commit.","title":"Status"},{"location":"tutorials/weights-and-biasis-logging/","text":"\ud83d\udcda This guide explains how to use Weights & Biases (W&B) with YOLOv5 \ud83d\ude80. About Weights & Biases Think of W&B like GitHub for machine learning models. With a few lines of code, save everything you need to debug, compare and reproduce your models \u2014 architecture, hyperparameters, git commits, model weights, GPU usage, and even datasets and predictions. W&B\u2019s lightweight integrations work with any Python script, and you can sign up for a free account and start tracking and visualizing models in 5 minutes. Used by top researchers including teams at OpenAI, Lyft, Github, and MILA, W&B is part of the new standard of best practices for machine learning. How W&B can help you optimize your machine learning workflows: Debug model performance in real time GPU usage , visualized automatically Custom charts for powerful, extensible visualization Share insights interactively with collaborators Optimize hyperparameters efficiently Track datasets, pipelines, and production models Before You Start Clone this repo and install requirements.txt dependencies, including Python>=3.8 and PyTorch>=1.7 . Also install the W&B pip package wandb . $ git clone https://github.com/ultralytics/yolov5 # clone repo $ cd yolov5 $ pip install -r requirements.txt wandb # install First-Time Setup When you first train, W&B will prompt you to create a new account and will generate an API key for you. If you are an existing user you can retrieve your key from https://wandb.ai/authorize. This key is used to tell W&B where to log your data. You only need to supply your key once, and then it is remembered on the same device. W&B will create a cloud project (default is 'YOLOv5') for your training runs, and each new training run will be provided a unique run name within that project as project/name. You can also manually set your project and run name as: $ python train.py --project ... --name ... Viewing Runs Run information streams from your environment to the W&B cloud console as you train. This allows you to monitor and even cancel runs in realtime . All important information is logged: Training losses Validation losses Metrics: Precision, Recall, mAP@0.5, mAP@0.5:0.95 Learning Rate over time GPU: Type, GPU Utilization , power, temperature, CUDA memory usage System: Disk I/0, CPU utilization, RAM memory usage Environment: OS and Python types, Git repository and state, training command Reports W&B Reports can be created from your saved runs for sharing online. Once a report is created you will receive a link you can use to publically share your results. Here is an example report created from the COCO128 tutorial trainings of all four YOLOv5 models ( link ). Environments YOLOv5 may be run in any of the following up-to-date verified environments (with all dependencies including CUDA / CUDNN , Python and PyTorch preinstalled): Google Colab and Kaggle notebooks with free GPU: Google Cloud Deep Learning VM. See GCP Quickstart Guide Amazon Deep Learning AMI. See AWS Quickstart Guide Docker Image . See Docker Quickstart Guide Status If this badge is green, all YOLOv5 GitHub Actions Continuous Integration (CI) tests are currently passing. CI tests verify correct operation of YOLOv5 training ( train.py ), testing ( test.py ), inference ( detect.py ) and export ( export.py ) on MacOS, Windows, and Ubuntu every 24 hours and on every commit.","title":"Weights & Biases Logging \ud83c\udd95"},{"location":"tutorials/weights-and-biasis-logging/#about-weights-biases","text":"Think of W&B like GitHub for machine learning models. With a few lines of code, save everything you need to debug, compare and reproduce your models \u2014 architecture, hyperparameters, git commits, model weights, GPU usage, and even datasets and predictions. W&B\u2019s lightweight integrations work with any Python script, and you can sign up for a free account and start tracking and visualizing models in 5 minutes. Used by top researchers including teams at OpenAI, Lyft, Github, and MILA, W&B is part of the new standard of best practices for machine learning. How W&B can help you optimize your machine learning workflows: Debug model performance in real time GPU usage , visualized automatically Custom charts for powerful, extensible visualization Share insights interactively with collaborators Optimize hyperparameters efficiently Track datasets, pipelines, and production models","title":"About Weights &amp; Biases"},{"location":"tutorials/weights-and-biasis-logging/#before-you-start","text":"Clone this repo and install requirements.txt dependencies, including Python>=3.8 and PyTorch>=1.7 . Also install the W&B pip package wandb . $ git clone https://github.com/ultralytics/yolov5 # clone repo $ cd yolov5 $ pip install -r requirements.txt wandb # install","title":"Before You Start"},{"location":"tutorials/weights-and-biasis-logging/#first-time-setup","text":"When you first train, W&B will prompt you to create a new account and will generate an API key for you. If you are an existing user you can retrieve your key from https://wandb.ai/authorize. This key is used to tell W&B where to log your data. You only need to supply your key once, and then it is remembered on the same device. W&B will create a cloud project (default is 'YOLOv5') for your training runs, and each new training run will be provided a unique run name within that project as project/name. You can also manually set your project and run name as: $ python train.py --project ... --name ...","title":"First-Time Setup"},{"location":"tutorials/weights-and-biasis-logging/#viewing-runs","text":"Run information streams from your environment to the W&B cloud console as you train. This allows you to monitor and even cancel runs in realtime . All important information is logged: Training losses Validation losses Metrics: Precision, Recall, mAP@0.5, mAP@0.5:0.95 Learning Rate over time GPU: Type, GPU Utilization , power, temperature, CUDA memory usage System: Disk I/0, CPU utilization, RAM memory usage Environment: OS and Python types, Git repository and state, training command","title":"Viewing Runs"},{"location":"tutorials/weights-and-biasis-logging/#reports","text":"W&B Reports can be created from your saved runs for sharing online. Once a report is created you will receive a link you can use to publically share your results. Here is an example report created from the COCO128 tutorial trainings of all four YOLOv5 models ( link ).","title":"Reports"},{"location":"tutorials/weights-and-biasis-logging/#environments","text":"YOLOv5 may be run in any of the following up-to-date verified environments (with all dependencies including CUDA / CUDNN , Python and PyTorch preinstalled): Google Colab and Kaggle notebooks with free GPU: Google Cloud Deep Learning VM. See GCP Quickstart Guide Amazon Deep Learning AMI. See AWS Quickstart Guide Docker Image . See Docker Quickstart Guide","title":"Environments"},{"location":"tutorials/weights-and-biasis-logging/#status","text":"If this badge is green, all YOLOv5 GitHub Actions Continuous Integration (CI) tests are currently passing. CI tests verify correct operation of YOLOv5 training ( train.py ), testing ( test.py ), inference ( detect.py ) and export ( export.py ) on MacOS, Windows, and Ubuntu every 24 hours and on every commit.","title":"Status"}]}